[
    {
        "ID": 1,
        "Title": "ICML 2023 Topological Deep Learning Challenge : Design and Results",
        "Authors": "Authors:\nMathilde Papillon, \n      \n      Mustafa Hajij, \n      \n      Helen Jenne, \n      \n      Johan Mathe, \n      \n      Audun Myers, \n      \n      Theodore Papamarkou, \n      \n      Tolga Birdal, \n      \n      Tamal Dey, \n      \n      Tim Doster, \n      \n      Tegan Emerson, \n      \n      Gurusankar Gopalakrishnan, \n      \n      Devendra Govil, \n      \n      Aldo Guzmán-Sáenz, \n      \n      Henry Kvinge, \n      \n      Neal Livesay, \n      \n      Soham Mukherjee, \n      \n      Shreyas N. Samaga, \n      \n      Karthikeyan Natesan Ramamurthy, \n      \n      Maneel Reddy Karri, \n      \n      Paul Rosen, \n      \n      Sophia Sanborn, \n      \n      Robin Walters, \n      \n      Jens Agerberg, \n      \n      Sadrodin Barikbin, \n      \n      Claudio Battiloro\n      , et al. (31 additional authors not shown)",
        "Abstract": "Abstract:\n      \n        …in Machine Learning. The competition asked participants to provide open-source implementations of topological neural networks from the literature by contributing to the python packages TopoNetX (data processing) and TopoModelX (deep learning). The challenge attracted twenty-eight qualifying submissions in its two-month duration. This paper describes the desi…\n        ▽ More\n\n\n        This paper presents the computational challenge on topological deep learning that was hosted within the ICML 2023 Workshop on Topology and Geometry in Machine Learning. The competition asked participants to provide open-source implementations of topological neural networks from the literature by contributing to the python packages TopoNetX (data processing) and TopoModelX (deep learning). The challenge attracted twenty-eight qualifying submissions in its two-month duration. This paper describes the design of the challenge and summarizes its main findings.\n        △ Less",
        "Date": "Submitted 18 January, 2024; v1 submitted 26 September, 2023;\n      originally announced September 2023."
    },
    {
        "ID": 2,
        "Title": "Determining Optimal Lot Size, Reorder Point, and Quality Features for a Food Item in a Cold Warehouse: Data-Driven Optimization Approach",
        "Authors": "Authors:\nAtena Karimi, \n      \n      Omid Ghorbani, \n      \n      Reza Tashakkori, \n      \n      Seyed Hamid Reza Pasandideh, \n      \n      Milad Jasemi",
        "Abstract": "Abstract:\n      \n        …model seeks to minimize the annual total cost of managing the warehouse. The model will be a nonlinear mixed programming one, which is solved by Pyomo as a leading library in Python language programming. Numerical examples are used to demonstrate the use of the model and, through sensitivity analysis, develop insights into the operation of cold warehouses. T…\n        ▽ More\n\n\n        We propose a nonlinear optimization model for determining the optimum lot size and reorder point for a food item distributed through a cold warehouse as well as the optimum quality features, namely temperature, humidity, packaging type, and level of environmental conditions. The item's quality is estimated based on the features mentioned earlier, and then it is used as a constraint in the optimization process. An assumption was made that the inventory is managed under a continuous review policy and the warehouse has limited space. The model seeks to minimize the annual total cost of managing the warehouse. The model will be a nonlinear mixed programming one, which is solved by Pyomo as a leading library in Python language programming. Numerical examples are used to demonstrate the use of the model and, through sensitivity analysis, develop insights into the operation of cold warehouses. This sensitive analysis opens the doors to managerial insight from which managers and policymakers can highly benefit.\n        △ Less",
        "Date": "Submitted 18 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 3,
        "Title": "Projection-based Prediction-Correction Method for Distributed Consensus Optimization",
        "Authors": "Authors:\nHan Long",
        "Abstract": "Abstract:\n      \n        …Upon applying the method to distributed linear least squares problems, it manifested a performance superiority, registering an enhancement exceeding 55% compared to Python's built-in functions. Overall, the research provides a robust distributed optimization technique with significant theoretical and practical benefits.\n        ▽ More\n\n\n        In the industrial technology domain, mathematical optimization is crucial with its applications seen in areas like transportation engineering, robotics, and machine learning. With the growth in data volume, there's an increased demand for solutions to large-scale challenges, leading to the rise of distributed optimization. This approach involves decentralized devices working collectively to achieve system objectives. The focus of the study is on distributed consensus optimization concerning convex set constraints in networks. The paper introduces the self-adaptive Projection-based Prediction-Correction Method (PPCM), inspired by the proximal method and integrated with variational inequality. PPCM stands out as a contractive method characterized by impressive convergence properties. Its decentralized nature also fits networked settings aptly. Also the parameter selection is simple and clear, without the hassle of parameter tuning. A thorough theoretical evaluation confirms PPCM's effectiveness. Upon applying the method to distributed linear least squares problems, it manifested a performance superiority, registering an enhancement exceeding 55% compared to Python's built-in functions. Overall, the research provides a robust distributed optimization technique with significant theoretical and practical benefits.\n        △ Less",
        "Date": "Submitted 18 January, 2024; v1 submitted 18 September, 2023;\n      originally announced September 2023."
    },
    {
        "ID": 4,
        "Title": "A Quick Primer on Machine Learning in Wireless Communications",
        "Authors": "Authors:\nFaris B. Mismar",
        "Abstract": "Abstract:\n      \n        This is a first draft of a quick primer on the use of Python (and relevant libraries) to build a wireless communication prototype that supports multiple-input and multiple-output (MIMO) systems with orthogonal frequency division multiplexing (OFDM) in addition to some machine learning use cases. This primer is intended to empower researchers with a means to…\n        ▽ More\n\n\n        This is a first draft of a quick primer on the use of Python (and relevant libraries) to build a wireless communication prototype that supports multiple-input and multiple-output (MIMO) systems with orthogonal frequency division multiplexing (OFDM) in addition to some machine learning use cases. This primer is intended to empower researchers with a means to efficiently create simulations. This draft is aligned with the syllabus of a graduate course we created to be taught in Fall 2022 and we aspire to update this draft occasionally based on feedback from the larger research community.\n        △ Less",
        "Date": "Submitted 17 January, 2024; v1 submitted 29 December, 2023;\n      originally announced December 2023."
    },
    {
        "ID": 5,
        "Title": "eipy: An Open-Source Python Package for Multi-modal Data Integration using Heterogeneous Ensembles",
        "Authors": "Authors:\nJamie J. R. Bennett, \n      \n      Yan Chak Li, \n      \n      Gaurav Pandey",
        "Abstract": "Abstract:\n      \n        In this paper, we introduce eipy--an open-source Python package for developing effective, multi-modal heterogeneous ensembles for classification. eipy simultaneously provides both a rigorous, and user-friendly framework for comparing and selecting the best-performing multi-modal data integration and predictive modeling methods by systematically evaluating th…\n        ▽ More\n\n\n        In this paper, we introduce eipy--an open-source Python package for developing effective, multi-modal heterogeneous ensembles for classification. eipy simultaneously provides both a rigorous, and user-friendly framework for comparing and selecting the best-performing multi-modal data integration and predictive modeling methods by systematically evaluating their performance using nested cross-validation. The package is designed to leverage scikit-learn-like estimators as components to build multi-modal predictive models. An up-to-date user guide, including API reference and tutorials, for eipy is maintained at https://eipy.readthedocs.io . The main repository for this project can be found on GitHub at https://github.com/GauravPandeyLab/eipy .\n        △ Less",
        "Date": "Submitted 17 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 6,
        "Title": "SARRIGUREN: a polynomial-time complete algorithm for random $k$-SAT with relatively dense clauses",
        "Authors": "Authors:\nAlfredo Goñi Sarriguren",
        "Abstract": "Abstract:\n      \n        …and dense clauses is not harder than 3-SAT. Moreover, the Python implementation of the algorithms, and all the input datasets and obtained results in the experiments are made available.\n        ▽ More\n\n\n        SARRIGUREN, a new complete algorithm for SAT based on counting clauses (which is valid also for Unique-SAT and #SAT) is described, analyzed and tested. Although existing complete algorithms for SAT perform slower with clauses with many literals, that is an advantage for SARRIGUREN, because the more literals are in the clauses the bigger is the probability of overlapping among clauses, a property that makes the clause counting process more efficient. Actually, it provides a $O(m^2 \\times n/k)$ time complexity for random $k$-SAT instances of $n$ variables and $m$ relatively dense clauses, where that density level is relative to the number of variables $n$, that is, clauses are relatively dense when $k\\geq7\\sqrt{n}$. Although theoretically there could be worst-cases with exponential complexity, the probability of those cases to happen in random $k$-SAT with relatively dense clauses is practically zero. The algorithm has been empirically tested and that polynomial time complexity maintains also for $k$-SAT instances with less dense clauses ($k\\geq5\\sqrt{n}$). That density could, for example, be of only 0.049 working with $n=20000$ variables and $k=989$ literals. In addition, they are presented two more complementary algorithms that provide the solutions to $k$-SAT instances and valuable information about number of solutions for each literal. Although this algorithm does not solve the NP=P problem (it is not a polynomial algorithm for 3-SAT), it broads the knowledge about that subject, because $k$-SAT with $k>3$ and dense clauses is not harder than 3-SAT. Moreover, the Python implementation of the algorithms, and all the input datasets and obtained results in the experiments are made available.\n        △ Less",
        "Date": "Submitted 17 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 7,
        "Title": "CoTran: An LLM-based Code Translator using Reinforcement Learning with Feedback from Compiler and Symbolic Execution",
        "Authors": "Authors:\nPrithwish Jana, \n      \n      Piyush Jha, \n      \n      Haoyang Ju, \n      \n      Gautham Kishore, \n      \n      Aryan Mahajan, \n      \n      Vijay Ganesh",
        "Abstract": "Abstract:\n      \n        …comparing CoTran with 14 other code translation tools that include human-written transpilers, LLM-based translation tools, and ChatGPT over a benchmark of more than 57,000 Java-Python equivalent pairs, and we show that CoTran outperforms them on relevant metrics such as compilation accuracy (CompAcc) and functional equivalence accuracy (FEqAcc). For example,…\n        ▽ More\n\n\n        In this paper, we present an LLM-based code translation method and an associated tool called CoTran, that translates whole-programs from one high-level programming language to another. Current LLM-based code translation methods lack a training approach to ensure that the translated code reliably compiles or bears substantial functional equivalence to the input code. In our work, we train an LLM via reinforcement learning, by modifying the fine-tuning process to incorporate compiler feedback and symbolic execution (symexec)-based equivalence testing feedback that checks for functional equivalence between the input and output programs. The idea is to guide an LLM-in-training, via compiler and symexec-based testing feedback, by letting it know how far it is from producing perfect translations. We report on extensive experiments comparing CoTran with 14 other code translation tools that include human-written transpilers, LLM-based translation tools, and ChatGPT over a benchmark of more than 57,000 Java-Python equivalent pairs, and we show that CoTran outperforms them on relevant metrics such as compilation accuracy (CompAcc) and functional equivalence accuracy (FEqAcc). For example, our tool achieves 48.68% FEqAcc, 76.98% CompAcc for Python-to-Java translation, whereas the nearest competing tool (PLBART-base) only gets 38.26% and 75.77% resp. Also, built upon CodeT5, CoTran achieves +11.23%, +14.89% improvement on FEqAcc and +4.07%, +8.14% on CompAcc for Java-to-Python and Python-to-Java translation resp.\n        △ Less",
        "Date": "Submitted 16 January, 2024; v1 submitted 11 June, 2023;\n      originally announced June 2023."
    },
    {
        "ID": 8,
        "Title": "Scalable hierarchical BayeSN inference: Investigating dependence of SN Ia host galaxy dust properties on stellar mass and redshift",
        "Authors": "Authors:\nMatthew Grayling, \n      \n      Stephen Thorp, \n      \n      Kaisey S. Mandel, \n      \n      Suhail Dhawan, \n      \n      Ana Sofia Uzsoy, \n      \n      Benjamin M. Boyd, \n      \n      Erin E. Hayesn, \n      \n      Sam M. Ward",
        "Abstract": "Abstract:\n      \n        …. In addition, we discuss in brief a new, GPU-accelerated Python implementation of BayeSN suitable for application to large surveys which is publicly available and can be used for future cosmological analyses; this code can be found here: https://github.com/bayesn/bayesn.\n        ▽ More\n\n\n        We apply the hierarchical probabilistic SED model BayeSN to analyse a sample of 475 SNe Ia (0.015 < z < 0.4) from Foundation, DES3YR and PS1MD to investigate the properties of dust in their host galaxies. We jointly infer the dust law $R_V$ population distributions at the SED level in high- and low-mass galaxies simultaneously with dust-independent, intrinsic differences. We find an intrinsic mass step of $-0.049\\pm0.016$ mag, at a significance of 3.1$σ$, when allowing for a constant intrinsic, achromatic magnitude offset. We additionally apply a model allowing for time- and wavelength-dependent intrinsic differences between SNe Ia in different mass bins, finding $\\sim$2$σ$ differences in magnitude and colour around peak and 4.5$σ$ differences at later times. These intrinsic differences are inferred simultaneously with a difference in population mean $R_V$ of $\\sim$2$σ$ significance, demonstrating that both intrinsic and extrinsic differences may play a role in causing the host galaxy mass step. We also consider a model which allows the mean of the $R_V$ distribution to linearly evolve with redshift but find no evidence for any evolution - we infer the gradient of this relation $η_R = -0.38\\pm0.70$. In addition, we discuss in brief a new, GPU-accelerated Python implementation of BayeSN suitable for application to large surveys which is publicly available and can be used for future cosmological analyses; this code can be found here: https://github.com/bayesn/bayesn.\n        △ Less",
        "Date": "Submitted 16 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 9,
        "Title": "SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking",
        "Authors": "Authors:\nSoukayna Mouatadid, \n      \n      Paulo Orenstein, \n      \n      Genevieve Flaspohler, \n      \n      Miruna Oprescu, \n      \n      Judah Cohen, \n      \n      Franklyn Wang, \n      \n      Sean Knight, \n      \n      Maria Geogdzhayeva, \n      \n      Sam Levang, \n      \n      Ernest Fraenkel, \n      \n      Lester Mackey",
        "Abstract": "Abstract:\n      \n        …ways to extend the accuracy of current operational models. SubseasonalClimateUSA is regularly updated and accessible via the https://github.com/microsoft/subseasonal_data/ Python package.\n        ▽ More\n\n\n        Subseasonal forecasting of the weather two to six weeks in advance is critical for resource allocation and advance disaster notice but poses many challenges for the forecasting community. At this forecast horizon, physics-based dynamical models have limited skill, and the targets for prediction depend in a complex manner on both local weather variables and global climate variables. Recently, machine learning methods have shown promise in advancing the state of the art but only at the cost of complex data curation, integrating expert knowledge with aggregation across multiple relevant data sources, file formats, and temporal and spatial resolutions. To streamline this process and accelerate future development, we introduce SubseasonalClimateUSA, a curated dataset for training and benchmarking subseasonal forecasting models in the United States. We use this dataset to benchmark a diverse suite of models, including operational dynamical models, classical meteorological baselines, and ten state-of-the-art machine learning and deep learning-based methods from the literature. Overall, our benchmarks suggest simple and effective ways to extend the accuracy of current operational models. SubseasonalClimateUSA is regularly updated and accessible via the https://github.com/microsoft/subseasonal_data/ Python package.\n        △ Less",
        "Date": "Submitted 16 January, 2024; v1 submitted 21 September, 2021;\n      originally announced September 2021."
    },
    {
        "ID": 10,
        "Title": "divERGe implements various Exact Renormalization Group examples",
        "Authors": "Authors:\nJonas B. Hauck, \n      \n      Dante M. Kennes, \n      \n      Lennart Klebl",
        "Abstract": "Abstract:\n      \n        We present divERGe, an open source, high-performance C/C++/Python library for functional renormalization group (FRG) calculations on lattice fermions. The versatile model interface is tailored to real materials applications and seamlessly integrates with existing, standard tools from the ab-initio community. The code fully supports multi-site, multi-orbital,…\n        ▽ More\n\n\n        We present divERGe, an open source, high-performance C/C++/Python library for functional renormalization group (FRG) calculations on lattice fermions. The versatile model interface is tailored to real materials applications and seamlessly integrates with existing, standard tools from the ab-initio community. The code fully supports multi-site, multi-orbital, and non-SU(2) models in all of the three included FRG variants: TU$^2$FRG, N-patch FRG, and grid FRG. With this, the divERGe library paves the way for widespread application of FRG as a tool in the study of competing orders in quantum materials.\n        △ Less",
        "Date": "Submitted 16 January, 2024; v1 submitted 13 November, 2023;\n      originally announced November 2023."
    },
    {
        "ID": 11,
        "Title": "Streamline-Directed Tunable Deterministic Lateral Displacement (DLD) Chip: A Novel Approach to Efficient Particle Separation",
        "Authors": "Authors:\nAli Kheirkhah Barzoki, \n      \n      Amir Shamloo",
        "Abstract": "Abstract:\n      \n        …of the particles. By changing the angle of streamlines with pillar array, the Dc can be tuned. Prior to determining the Dc for each case, an initial estimation was made using a Python script that utilized the streamline coordinates. Subsequently, through FEM modeling of the particle trajectories, precise Dc values were ascertained and juxtaposed with the est…\n        ▽ More\n\n\n        In conventional Deterministic Lateral Displacement (DLD), the migration behavior of a particle of specific size is determined by the critical diameter (Dc), which is predefined by the device's geometry. In contrast to the typical approach that alters the angle between the pillar array and fluid streamlines by modifying the geometrical parameters, this study introduces a novel perspective that focuses on changing the direction of the streamlines. The proposed technique enables the fabrication of a tunable DLD chip that is both easy to manufacture and design. This chip features one completely horizontal pillar array with two bypass channels on the top and bottom of the DLD chamber. The width of these bypass channels changes linearly from their inlet to their outlet. Two design configurations are suggested for this chip, characterized by either parallel or unparallel slopes of the bypass channels. This chip is capable of generating a wide range of Dc values by manipulating two distinct control parameters. The first control parameter involves adjusting the flow rates in the two bypass channels. The second control parameter entails controlling the slopes of these bypass channels. Both of these parameters influence the direction of particle-carrying streamlines resulting in a change in the path-line of the particles. By changing the angle of streamlines with pillar array, the Dc can be tuned. Prior to determining the Dc for each case, an initial estimation was made using a Python script that utilized the streamline coordinates. Subsequently, through FEM modeling of the particle trajectories, precise Dc values were ascertained and juxtaposed with the estimated values, revealing minimal disparities. This innovative chip enables the attainment of Dc values spanning from 0.5 to 14 μm.\n        △ Less",
        "Date": "Submitted 16 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 12,
        "Title": "Deep learning based Image Compression for Microscopy Images: An Empirical Study",
        "Authors": "Authors:\nYu Zhou, \n      \n      Jan Sollmann, \n      \n      Jianxu Chen",
        "Abstract": "Abstract:\n      \n        …wanted way, multiple classical lossy image compression techniques are compared to several AI-based compression models provided by and trained with the CompressAI toolbox using python. These different compression techniques are compared in compression ratio, multiple image similarity measures and, most importantly, the prediction accuracy from label-free mode…\n        ▽ More\n\n\n        With the fast development of modern microscopes and bioimaging techniques, an unprecedentedly large amount of imaging data are being generated, stored, analyzed, and even shared through networks. The size of the data poses great challenges for current data infrastructure. One common way to reduce the data size is by image compression. This present study analyzes classic and deep learning based image compression methods, and their impact on deep learning based image processing models. Deep learning based label-free prediction models (i.e., predicting fluorescent images from bright field images) are used as an example application for comparison and analysis. Effective image compression methods could help reduce the data size significantly without losing necessary information, and therefore reduce the burden on data management infrastructure and permit fast transmission through the network for data sharing or cloud computing. To compress images in such a wanted way, multiple classical lossy image compression techniques are compared to several AI-based compression models provided by and trained with the CompressAI toolbox using python. These different compression techniques are compared in compression ratio, multiple image similarity measures and, most importantly, the prediction accuracy from label-free models on compressed images. We found that AI-based compression techniques largely outperform the classic ones and will minimally affect the downstream label-free task in 2D cases. In the end, we hope the present study could shed light on the potential of deep learning based image compression and the impact of image compression on downstream deep learning based image analysis models.\n        △ Less",
        "Date": "Submitted 16 January, 2024; v1 submitted 2 November, 2023;\n      originally announced November 2023."
    },
    {
        "ID": 13,
        "Title": "Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code",
        "Authors": "Authors:\nRangeet Pan, \n      \n      Ali Reza Ibrahimzada, \n      \n      Rahul Krishna, \n      \n      Divya Sankar, \n      \n      Lambert Pouguem Wassi, \n      \n      Michele Merler, \n      \n      Boris Sobolev, \n      \n      Raju Pavuluri, \n      \n      Saurabh Sinha, \n      \n      Reyhaneh Jabbarvand",
        "Abstract": "Abstract:\n      \n        …a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate co…\n        ▽ More\n\n\n        Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation -- with correct translations ranging from 2.1% to 47.3% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset -- consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs -- can help drive research in this area.\n        △ Less",
        "Date": "Submitted 16 January, 2024; v1 submitted 6 August, 2023;\n      originally announced August 2023."
    },
    {
        "ID": 14,
        "Title": "SOQCS: A Stochastic Optical Quantum Circuit Simulator",
        "Authors": "Authors:\nJavier Osca, \n      \n      Jiri Vala",
        "Abstract": "Abstract:\n      \n        We present Stochastic Optical Quantum Circuit Simulator (SOQCS) C++/Python library for the simulation of quantum optical circuits, and we provide its implementation details. SOQCS offers a framework to define, simulate and study quantum linear optical circuits in the presence of various imperfections. These come from partial distinguishability of photons, lo…\n        ▽ More\n\n\n        We present Stochastic Optical Quantum Circuit Simulator (SOQCS) C++/Python library for the simulation of quantum optical circuits, and we provide its implementation details. SOQCS offers a framework to define, simulate and study quantum linear optical circuits in the presence of various imperfections. These come from partial distinguishability of photons, lossy propagation media, unbalanced beamsplitters and non-ideal emitters and detectors for example. SOQCS is developed as a series of different modules which provide quantum circuits, different simulator cores and tools to analyze the output. Quantum circuits can be defined from basic components, including emitters, linear optical elements, delays and detectors. Post-selection can be configured straightforwardly as part of detector definitions. An important attribute of SOQCS is its modularity which allows for its further development in the future.\n        △ Less",
        "Date": "Submitted 16 January, 2024; v1 submitted 13 July, 2023;\n      originally announced July 2023."
    },
    {
        "ID": 15,
        "Title": "MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline",
        "Authors": "Authors:\nMinpeng Liao, \n      \n      Wei Luo, \n      \n      Chengxi Li, \n      \n      Jing Wu, \n      \n      Kai Fan",
        "Abstract": "Abstract:\n      \n        …theoretical standpoints. In this paper, we address this challenge by enriching the data landscape and introducing a novel math dataset, enhanced with a capability to utilize a Python code interpreter. This dataset is derived from GSM8K and MATH and has been further refined through a combination of GPT-4 annotations, human review, and self-training processes,…\n        ▽ More\n\n\n        Large language models (LLMs) have seen considerable advancements in natural language understanding tasks, yet there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints. In this paper, we address this challenge by enriching the data landscape and introducing a novel math dataset, enhanced with a capability to utilize a Python code interpreter. This dataset is derived from GSM8K and MATH and has been further refined through a combination of GPT-4 annotations, human review, and self-training processes, where the errors in the original GSM8K training set have been fixed. Additionally, we propose a tentative, easily replicable protocol for the fine-tuning of math-specific LLMs, which has led to a significant improvement in the performance of a 7B-parameter LLM on the GSM8K and MATH datasets. We are committed to advancing the field of mathematical reasoning in LLMs and, to that end, we have made the model checkpoints and will make the dataset publicly available. We hope this will facilitate further research and development within the community.\n        △ Less",
        "Date": "Submitted 16 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 16,
        "Title": "CodeComplex: A Time-Complexity Dataset for Bilingual Source Codes",
        "Authors": "Authors:\nSeung-Yeop Baik, \n      \n      Mingi Jeon, \n      \n      Joonghyuk Hahn, \n      \n      Jungin Kim, \n      \n      Yo-Sub Han, \n      \n      Sang-Ki Ko",
        "Abstract": "Abstract:\n      \n        …source code dataset where each code is manually annotated with a corresponding worst-case time complexity. CodeComplex comprises 4,900 Java codes and an equivalent number of Python codes, all sourced from programming competitions and annotated with complexity labels by a panel of algorithmic experts. To the best of our knowledge, CodeComplex stands as the mo…\n        ▽ More\n\n\n        Analyzing the worst-case time complexity of a code is a crucial task in computer science and software engineering for ensuring the efficiency, reliability, and robustness of software systems. However, it is well-known that the problem of determining the worst-case time complexity of a given code written in general-purpose programming language is theoretically undecidable by the famous Halting problem proven by Alan Turing. Thus, we move towards more realistic scenarios where the inputs and outputs of a program exist. This allows us to discern the correctness of given codes, challenging to analyze their time complexity exhaustively. In response to this challenge, we introduce CodeComplex, a novel source code dataset where each code is manually annotated with a corresponding worst-case time complexity. CodeComplex comprises 4,900 Java codes and an equivalent number of Python codes, all sourced from programming competitions and annotated with complexity labels by a panel of algorithmic experts. To the best of our knowledge, CodeComplex stands as the most extensive code dataset tailored for predicting complexity. Subsequently, we present the outcomes of our experiments employing various baseline models, leveraging state-of-the-art neural models in code comprehension like CodeBERT, GraphCodeBERT, UniXcoder, PLBART, CodeT5, CodeT5+, and ChatGPT. We analyze how the dataset impacts the model's learning in predicting time complexity.\n        △ Less",
        "Date": "Submitted 16 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 17,
        "Title": "Maunakea Spectroscopic Explorer exposure time calculator for end-to-end simulator: to optimizing spectrograph design and observing simulation",
        "Authors": "Authors:\nTae-Geun Ji, \n      \n      Jennifer Sobeck, \n      \n      Changgon Kim, \n      \n      Hojae Ahn, \n      \n      Mingyeong Yang, \n      \n      Taeeun Kim, \n      \n      Sungwook E. Hong, \n      \n      Kei Szeto, \n      \n      Jennifer L. Marshall, \n      \n      Christian Surace, \n      \n      Soojong Pak",
        "Abstract": "Abstract:\n      \n        …of target AB magnitude, water vapor, airmass, and sky brightness AB magnitude (additional user inputs can be provided depending on computational mode). The ETC is built using Python 3.7 and features a graphical user interface that allows for cross-platform use. The development process of the ETC software follows an Agile methodology and utilizes the Unified…\n        ▽ More\n\n\n        The Maunakea Spectroscopic Explorer (MSE) project will provide multi-object spectroscopy in the optical and near-infrared bands using an 11.25-m aperture telescope, repurposing the original Canada-France-Hawaii Telescope (CFHT) site. MSE will observe 4,332 objects per single exposure with a field of view of 1.5 square degrees, utilizing two spectrographs with low-moderate (R$\\sim$3,000, 6,000) and high (R$\\approx$30,000) spectral resolution. In general, an exposure time calculator (ETC) is used to estimate the performance of an observing system by calculating a signal-to-noise ratio (S/N) and exposure time. We present the design of the MSE exposure time calculator (ETC), which has four calculation modes (S/N, exposure time, S/N trend with wavelength, and S/N trend with magnitude) and incorporates the MSE system requirements as specified in the Conceptual Design. The MSE ETC currently allows for user-defined inputs of target AB magnitude, water vapor, airmass, and sky brightness AB magnitude (additional user inputs can be provided depending on computational mode). The ETC is built using Python 3.7 and features a graphical user interface that allows for cross-platform use. The development process of the ETC software follows an Agile methodology and utilizes the Unified Modeling Language (UML) diagrams to visualize the software architecture. We also describe the testing and verification of the MSE ETC.\n        △ Less",
        "Date": "Submitted 16 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 18,
        "Title": "MCMChaos: Improvising Rap Music with MCMC Methods and Chaos Theory",
        "Authors": "Authors:\nRobert G. Kimelman",
        "Abstract": "Abstract:\n      \n        …gibbs sampler and lorenz attractor simulation. As far as we know, these simulation methods have never been used in rap music generation before. The software implements Python Text-to-Speech processing (pyttxs) to convert text wrangled from the MCFlow corpus into English speech. In each version, values simulated from each respective mathematical model alter t…\n        ▽ More\n\n\n        A novel freestyle rap software, MCMChaos 0.0.1, based on rap music transcriptions created in previous research is presented. The software has three different versions, each making use of different mathematical simulation methods: collapsed gibbs sampler and lorenz attractor simulation. As far as we know, these simulation methods have never been used in rap music generation before. The software implements Python Text-to-Speech processing (pyttxs) to convert text wrangled from the MCFlow corpus into English speech. In each version, values simulated from each respective mathematical model alter the rate of speech, volume, and (in the multiple voice case) the voice of the text-to-speech engine on a line-by-line basis. The user of the software is presented with a real-time graphical user interface (GUI) which instantaneously changes the initial values read into the mathematical simulation methods. Future research might attempt to allow for more user control and autonomy.\n        △ Less",
        "Date": "Submitted 15 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 19,
        "Title": "Fuzz4All: Universal Fuzzing with Large Language Models",
        "Authors": "Authors:\nChunqiu Steven Xia, \n      \n      Matteo Paltenghi, \n      \n      Jia Le Tian, \n      \n      Michael Pradel, \n      \n      Lingming Zhang",
        "Abstract": "Abstract:\n      \n        …iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4All on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4All has ide…\n        ▽ More\n\n\n        Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language. To realize this potential, we present a novel autoprompting technique, which creates LLM prompts that are wellsuited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4All on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4All has identified 98 bugs in widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit quantum computing platform, with 64 bugs already confirmed by developers as previously unknown.\n        △ Less",
        "Date": "Submitted 15 January, 2024; v1 submitted 9 August, 2023;\n      originally announced August 2023."
    },
    {
        "ID": 20,
        "Title": "JumpCoder: Go Beyond Autoregressive Coder via Online Modification",
        "Authors": "Authors:\nMouxiang Chen, \n      \n      Hao Tian, \n      \n      Zhongxin Liu, \n      \n      Xiaoxue Ren, \n      \n      Jianling Sun",
        "Abstract": "Abstract:\n      \n        …multiple benchmarks consistently indicate significant improvements over all baselines. Notably, JumpCoder assists code LLMs in achieving up to a 3.6% increase in Pass@1 for Python, 6.3% for Java, and 3.7% for C++ in the multilingual HumanEval benchmarks. Our code is public at https://github.com/Keytoyze/JumpCoder.\n        ▽ More\n\n\n        While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance. We introduce JumpCoder, a novel modelagnostic framework that enables online modification and non-sequential generation to augment the code LLMs. The key idea behind JumpCoder is to insert new code into the currently generated code when necessary during generation, which is achieved through an auxiliary infilling model that works in tandem with the code LLM. Since identifying the best infill position beforehand is intractable, we adopt an infill-first, judge-later strategy, which experiments with filling at the $k$ most critical positions following the generation of each line, and uses an Abstract Syntax Tree (AST) parser alongside the Generation Model Scoring to effectively judge the validity of each potential infill. Extensive experiments using six state-of-the-art code LLMs across multiple benchmarks consistently indicate significant improvements over all baselines. Notably, JumpCoder assists code LLMs in achieving up to a 3.6% increase in Pass@1 for Python, 6.3% for Java, and 3.7% for C++ in the multilingual HumanEval benchmarks. Our code is public at https://github.com/Keytoyze/JumpCoder.\n        △ Less",
        "Date": "Submitted 15 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 21,
        "Title": "The Paradox of Function Header Comments",
        "Authors": "Authors:\nArthur Oxenhorn, \n      \n      Almog Mor, \n      \n      Uri Stern, \n      \n      Dror G. Feitelson",
        "Abstract": "Abstract:\n      \n        …any real information. We define a simple metric for information-less documentation based on its similarity to the function signature. Applying this to 21,140 files in GitHub Python projects shows that most functions are undocumented, but when header comments are written they typically do contain additional information beyond the function signature.\n        ▽ More\n\n\n        Perhaps the most widely used form of code documentation is function header comments. We performed a large-scale survey of 367 developers to catalog their expectations from such documentation and to chronicle actual practice. Paradoxically, we found that developers appreciate the value of header comments and estimate that they are worth the investment in time, but nevertheless they tend not to write such documentation in their own code. Reasons for not writing header comments vary from the belief that code should be self-documenting to concern that documentation will not be kept up-to-date. A possible outcome of this situation is that developers may evade requirements to write documentation by using templates to generate worthless comments that do not provide any real information. We define a simple metric for information-less documentation based on its similarity to the function signature. Applying this to 21,140 files in GitHub Python projects shows that most functions are undocumented, but when header comments are written they typically do contain additional information beyond the function signature.\n        △ Less",
        "Date": "Submitted 15 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 22,
        "Title": "Towards Automatic Translation of Machine Learning Visual Insights to Analytical Assertions",
        "Authors": "Authors:\nArumoy Shome, \n      \n      Luis Cruz, \n      \n      Arie van Deursen",
        "Abstract": "Abstract:\n      \n        We present our vision for developing an automated tool capable of translating visual properties observed in Machine Learning (ML) visualisations into Python assertions. The tool aims to streamline the process of manually verifying these visualisations in the ML development cycle, which is critical as real-world data and assumptions often change post-deployme…\n        ▽ More\n\n\n        We present our vision for developing an automated tool capable of translating visual properties observed in Machine Learning (ML) visualisations into Python assertions. The tool aims to streamline the process of manually verifying these visualisations in the ML development cycle, which is critical as real-world data and assumptions often change post-deployment. In a prior study, we mined $54,070$ Jupyter notebooks from Github and created a catalogue of $269$ semantically related visualisation-assertion (VA) pairs. Building on this catalogue, we propose to build a taxonomy that organises the VA pairs based on ML verification tasks. The input feature space comprises of a rich source of information mined from the Jupyter notebooks -- visualisations, Python source code, and associated markdown text. The effectiveness of various AI models, including traditional NLP4Code models and modern Large Language Models, will be compared using established machine translation metrics and evaluated through a qualitative study with human participants. The paper also plans to address the challenge of extending the existing VA pair dataset with additional pairs from Kaggle and to compare the tool's effectiveness with commercial generative AI models like ChatGPT. This research not only contributes to the field of ML system validation but also explores novel ways to leverage AI for automating and enhancing software engineering practices in ML.\n        △ Less",
        "Date": "Submitted 15 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 23,
        "Title": "Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement Learning",
        "Authors": "Authors:\nSotetsu Koyamada, \n      \n      Shinri Okano, \n      \n      Soichiro Nishimori, \n      \n      Yu Murata, \n      \n      Keigo Habara, \n      \n      Haruka Kita, \n      \n      Shin Ishii",
        "Abstract": "Abstract:\n      \n        …over accelerators. In our experiments on a DGX-A100 workstation, we discovered that Pgx can simulate RL environments 10-100x faster than existing implementations available in Python. Pgx includes RL environments commonly used as benchmarks in RL research, such as backgammon, chess, shogi, and Go. Additionally, Pgx offers miniature game sets and baseline mode…\n        ▽ More\n\n\n        We propose Pgx, a suite of board game reinforcement learning (RL) environments written in JAX and optimized for GPU/TPU accelerators. By leveraging JAX's auto-vectorization and parallelization over accelerators, Pgx can efficiently scale to thousands of simultaneous simulations over accelerators. In our experiments on a DGX-A100 workstation, we discovered that Pgx can simulate RL environments 10-100x faster than existing implementations available in Python. Pgx includes RL environments commonly used as benchmarks in RL research, such as backgammon, chess, shogi, and Go. Additionally, Pgx offers miniature game sets and baseline models to facilitate rapid research cycles. We demonstrate the efficient training of the Gumbel AlphaZero algorithm with Pgx environments. Overall, Pgx provides high-performance environment simulators for researchers to accelerate their RL experiments. Pgx is available at http://github.com/sotetsuk/pgx.\n        △ Less",
        "Date": "Submitted 15 January, 2024; v1 submitted 28 March, 2023;\n      originally announced March 2023."
    },
    {
        "ID": 24,
        "Title": "A Smooth Binary Mechanism for Efficient Private Continual Observation",
        "Authors": "Authors:\nJoel Daniel Andersson, \n      \n      Rasmus Pagh",
        "Abstract": "Abstract:\n      \n        …time per value, 2) the variance is reduced by a factor about 4 compared to the binary mechanism, and 3) the noise distribution at each step is identical. Empirically, a simple Python implementation of our approach outperforms the running time of the approach of Henzinger et al., as well as an attempt to improve their algorithm using high-performance algorith…\n        ▽ More\n\n\n        In privacy under continual observation we study how to release differentially private estimates based on a dataset that evolves over time. The problem of releasing private prefix sums of $x_1,x_2,x_3,\\dots \\in\\{0,1\\}$ (where the value of each $x_i$ is to be private) is particularly well-studied, and a generalized form is used in state-of-the-art methods for private stochastic gradient descent (SGD). The seminal binary mechanism privately releases the first $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently, Henzinger et al. and Denisov et al. showed that it is possible to improve on the binary mechanism in two ways: The variance of the noise can be reduced by a (large) constant factor, and also made more even across time steps. However, their algorithms for generating the noise distribution are not as efficient as one would like in terms of computation time and (in particular) space. We address the efficiency problem by presenting a simple alternative to the binary mechanism in which 1) generating the noise takes constant average time per value, 2) the variance is reduced by a factor about 4 compared to the binary mechanism, and 3) the noise distribution at each step is identical. Empirically, a simple Python implementation of our approach outperforms the running time of the approach of Henzinger et al., as well as an attempt to improve their algorithm using high-performance algorithms for multiplication with Toeplitz matrices.\n        △ Less",
        "Date": "Submitted 15 January, 2024; v1 submitted 16 June, 2023;\n      originally announced June 2023."
    },
    {
        "ID": 25,
        "Title": "Graph database while computationally efficient filters out quickly the ESG integrated equities in investment management",
        "Authors": "Authors:\nPartha Sen, \n      \n      Sumana Sen",
        "Abstract": "Abstract:\n      \n        …databases to compare and contrast efficiency and performance. To perform this experiment the data were collected from multiple sources including stock price and financial news. Python is used as an interface to connect and query databases (to create database structures according to the feed file structure, to load data into tables, objects, to read data , to…\n        ▽ More\n\n\n        Design/methodology/approach This research evaluated the databases of SQL, No-SQL and graph databases to compare and contrast efficiency and performance. To perform this experiment the data were collected from multiple sources including stock price and financial news. Python is used as an interface to connect and query databases (to create database structures according to the feed file structure, to load data into tables, objects, to read data , to connect PostgreSQL, ElasticSearch, Neo4j. Purpose Modern applications of LLM (Large language model) including RAG (Retrieval Augmented Generation) with Machine Learning, deep learning, NLP (natural language processing) or Decision Analytics are computationally expensive. Finding a better option to consume less resources and time to get the result. Findings The Graph database of ESG (Environmental, Social and Governance) is comparatively better and can be considered for extended analytics to integrate ESG in business and investment. Practical implications A graph ML with a RAG architecture model can be introduced as a new framework with less computationally expensive LLM application in the equity filtering process for portfolio management. Originality/value Filtering out selective stocks out of two thousand or more listed companies in any stock exchange for active investment, consuming less resource consumption especially memory and energy to integrate artificial intelligence and ESG in business and investment.\n        △ Less",
        "Date": "Submitted 15 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 26,
        "Title": "Lirot.ai: A Novel Platform for Crowd-Sourcing Retinal Image Segmentations",
        "Authors": "Authors:\nJonathan Fhima, \n      \n      Jan Van Eijgen, \n      \n      Moti Freiman, \n      \n      Ingeborg Stalmans, \n      \n      Joachim A. Behar",
        "Abstract": "Abstract:\n      \n        …image segmentations. Methods: Lirot. ai is composed of three components; an iPadOS client application named Lirot. ai-app, a backend server named Lirot. ai-server and a python API name Lirot. ai-API. Lirot. ai-app was developed in Swift 5.6 and Lirot. ai-server is a firebase backend. Lirot. ai-API allows the management of the database. Lirot. ai-app can be i…\n        ▽ More\n\n\n        Introduction: For supervised deep learning (DL) tasks, researchers need a large annotated dataset. In medical data science, one of the major limitations to develop DL models is the lack of annotated examples in large quantity. This is most often due to the time and expertise required to annotate. We introduce Lirot. ai, a novel platform for facilitating and crowd-sourcing image segmentations. Methods: Lirot. ai is composed of three components; an iPadOS client application named Lirot. ai-app, a backend server named Lirot. ai-server and a python API name Lirot. ai-API. Lirot. ai-app was developed in Swift 5.6 and Lirot. ai-server is a firebase backend. Lirot. ai-API allows the management of the database. Lirot. ai-app can be installed on as many iPadOS devices as needed so that annotators may be able to perform their segmentation simultaneously and remotely. We incorporate Apple Pencil compatibility, making the segmentation faster, more accurate, and more intuitive for the expert than any other computer-based alternative. Results: We demonstrate the usage of Lirot. ai for the creation of a retinal fundus dataset with reference vasculature segmentations. Discussion and future work: We will use active learning strategies to continue enlarging our retinal fundus dataset by including a more efficient process to select the images to be annotated and distribute them to annotators.\n        △ Less",
        "Date": "Submitted 14 January, 2024; v1 submitted 22 August, 2022;\n      originally announced August 2022."
    },
    {
        "ID": 27,
        "Title": "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges",
        "Authors": "Authors:\nKechi Zhang, \n      \n      Jia Li, \n      \n      Ge Li, \n      \n      Xianjie Shi, \n      \n      Zhi Jin",
        "Abstract": "Abstract:\n      \n        …-- real-world repo-level code generation. We introduce CodeAgentBench, a manually curated benchmark for repo-level code generation. This benchmark comprises five high-quality Python projects, encompassing a total of 101 samples. We assess nine leading LLMs on repo-level tasks and observe a decline in their performance. To tackle this, we present CodeAgent, a…\n        ▽ More\n\n\n        Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. Real-world software development, however, often involves complex code repositories (named repo) with complex dependencies and extensive documentation. To fill this gap, our research pivots towards evaluating LLMs in a more realistic setting -- real-world repo-level code generation. We introduce CodeAgentBench, a manually curated benchmark for repo-level code generation. This benchmark comprises five high-quality Python projects, encompassing a total of 101 samples. We assess nine leading LLMs on repo-level tasks and observe a decline in their performance. To tackle this, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code symbol navigation, and code testing. We implement four agent strategies to optimize these tools' usage. Our experiments on CodeAgentBench show that CodeAgent enhances LLM performance significantly, with improvements ranging from 18.1\\% to 250\\%. Further tests on the HumanEval benchmark confirm CodeAgent's adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent's robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.\n        △ Less",
        "Date": "Submitted 14 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 28,
        "Title": "Score-matching neural networks for improved multi-band source separation",
        "Authors": "Authors:\nMatt L. Sampson, \n      \n      Peter Melchior, \n      \n      Charlotte Ward, \n      \n      Sufia Birmingham",
        "Abstract": "Abstract:\n      \n        …while maintaining excellent performance for colors. We also demonstrate significant improvements in the robustness to inaccurate initializations. Scarlet2 is written in python, extendend by JAX and equinox, and is fully GPU compatible. The implementation and data package of the score model are publicly available at https://github.com/pmelchior/scarlet2.\n        ▽ More\n\n\n        We present the implementation of a score-matching neural network that represents a data-driven prior for non-parametric galaxy morphologies. The gradients of this prior can be included in the optimization routine of the recently developed multi-band modeling framework Scarlet2, a redesign of the Scarlet method currently employed as deblender in the pipelines of the HyperSuprimeCam survey and the Rubin Observatory. The addition of the prior avoids the requirement of nondifferentiable constraints, which can lead to convergence failures we discovered in Scarlet. We present the architecture and training details of our score-matching neural network and show with simulated Rubin-like observations that Scarlet2 outperforms Scarlet in accuracy of total flux and morphology estimates, while maintaining excellent performance for colors. We also demonstrate significant improvements in the robustness to inaccurate initializations. Scarlet2 is written in python, extendend by JAX and equinox, and is fully GPU compatible. The implementation and data package of the score model are publicly available at https://github.com/pmelchior/scarlet2.\n        △ Less",
        "Date": "Submitted 14 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 29,
        "Title": "Towards Responsible AI in Banking: Addressing Bias for Fair Decision-Making",
        "Authors": "Authors:\nAlessandro Castelnovo",
        "Abstract": "Abstract:\n      \n        …tools for the responsible implementation of AI-based decision-making systems. In line with open-source principles, we have released Bias On Demand and FairView as accessible Python packages, further promoting progress in the field of AI fairness.\n        ▽ More\n\n\n        In an era characterized by the pervasive integration of artificial intelligence into decision-making processes across diverse industries, the demand for trust has never been more pronounced. This thesis embarks on a comprehensive exploration of bias and fairness, with a particular emphasis on their ramifications within the banking sector, where AI-driven decisions bear substantial societal consequences. In this context, the seamless integration of fairness, explainability, and human oversight is of utmost importance, culminating in the establishment of what is commonly referred to as \"Responsible AI\". This emphasizes the critical nature of addressing biases within the development of a corporate culture that aligns seamlessly with both AI regulations and universal human rights standards, particularly in the realm of automated decision-making systems. Nowadays, embedding ethical principles into the development, training, and deployment of AI models is crucial for compliance with forthcoming European regulations and for promoting societal good. This thesis is structured around three fundamental pillars: understanding bias, mitigating bias, and accounting for bias. These contributions are validated through their practical application in real-world scenarios, in collaboration with Intesa Sanpaolo. This collaborative effort not only contributes to our understanding of fairness but also provides practical tools for the responsible implementation of AI-based decision-making systems. In line with open-source principles, we have released Bias On Demand and FairView as accessible Python packages, further promoting progress in the field of AI fairness.\n        △ Less",
        "Date": "Submitted 13 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 30,
        "Title": "Adaptoring: Adapter Generation to Provide an Alternative API for a Library",
        "Authors": "Authors:\nLars Reimann, \n      \n      Günter Kniesel-Wünsche",
        "Abstract": "Abstract:\n      \n        …API transformations. Finally, we consider the issue of migrating the generated adapters if the original library introduces breaking changes. We implemented our approach for Python, demonstrating its effectiveness to quickly provide an alternative API even for large libraries.\n        ▽ More\n\n\n        Third-party libraries are a cornerstone of fast application development. To enable efficient use, libraries must provide a well-designed API. An obscure API instead slows down the learning process and can lead to erroneous use.\n  The usual approach to improve the API of a library is to edit its code directly, either keeping the old API but deprecating it (temporarily increasing the API size) or dropping it (introducing breaking changes). If maintainers are unwilling to make such changes, others need to create a hard fork, which they can refactor. But then it is difficult to incorporate changes to the original library, such as bug fixes or performance improvements.\n  In this paper, we instead explore the use of the adapter pattern to provide a new API as a new library that calls the original library internally. This allows the new library to leverage all implementation changes to the original library, at no additional cost. We call this approach adaptoring. To make the approach practical, we identify API transformations for which adapter code can be generated automatically, and investigate which transformations can be inferred automatically, based on the documentation and usage patterns of the original library. For cases where automated inference is not possible, we present a tool that lets developers manually specify API transformations. Finally, we consider the issue of migrating the generated adapters if the original library introduces breaking changes. We implemented our approach for Python, demonstrating its effectiveness to quickly provide an alternative API even for large libraries.\n        △ Less",
        "Date": "Submitted 13 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 31,
        "Title": "Dynamic survival analysis: modelling the hazard function via ordinary differential equations",
        "Authors": "Authors:\nJ. A. Christen, \n      \n      F. J. Rubio",
        "Abstract": "Abstract:\n      \n        The hazard function represents one of the main quantities of interest in the analysis of survival data. We propose a general approach for parametrically modelling the dynamics of the hazard function using systems of autonomous ordinary differential equations (ODEs). This modelling approach can be used to provide qualitative and quantitative analyses of the evolution of the hazard function over tim…\n        ▽ More\n\n\n        The hazard function represents one of the main quantities of interest in the analysis of survival data. We propose a general approach for parametrically modelling the dynamics of the hazard function using systems of autonomous ordinary differential equations (ODEs). This modelling approach can be used to provide qualitative and quantitative analyses of the evolution of the hazard function over time. Our proposal capitalises on the extensive literature of ODEs which, in particular, allow for establishing basic rules or laws on the dynamics of the hazard function via the use of autonomous ODEs. We show how to implement the proposed modelling framework in cases where there is an analytic solution to the system of ODEs or where an ODE solver is required to obtain a numerical solution. We focus on the use of a Bayesian modelling approach, but the proposed methodology can also be coupled with maximum likelihood estimation. A simulation study is presented to illustrate the performance of these models and the interplay of sample size and censoring. Two case studies using real data are presented to illustrate the use of the proposed approach and to highlight the interpretability of the corresponding models. We conclude with a discussion on potential extensions of our work and strategies to include covariates into our framework.\n        △ Less",
        "Date": "Submitted 12 January, 2024; v1 submitted 9 August, 2023;\n      originally announced August 2023."
    },
    {
        "ID": 32,
        "Title": "Geometric Surprises in the Python's Lunch Conjecture",
        "Authors": "Authors:\nGurbir Arora, \n      \n      Matthew Headrick, \n      \n      Albion Lawrence, \n      \n      Martin Sasieta, \n      \n      Connor Wolfe",
        "Abstract": "Abstract:\n      \n        …slice of a holographic spacetime, is a non-minimal extremal surface that occurs between two locally minimal surfaces homologous to a given boundary region. According to the python's lunch conjecture of Brown et al., the bulge's area controls the complexity of bulk reconstruction, in the sense of the amount of post-selection that needs to be overcome…\n        ▽ More\n\n\n        A bulge surface, on a time reflection-symmetric Cauchy slice of a holographic spacetime, is a non-minimal extremal surface that occurs between two locally minimal surfaces homologous to a given boundary region. According to the python's lunch conjecture of Brown et al., the bulge's area controls the complexity of bulk reconstruction, in the sense of the amount of post-selection that needs to be overcome for the reconstruction of the entanglement wedge beyond the outermost extremal surface. We study the geometry of bulges in a variety of classical spacetimes, and discover a number of surprising features that distinguish them from more familiar extremal surfaces such as Ryu-Takayanagi surfaces: they spontaneously break spatial isometries, both continuous and discrete; they are sensitive to the choice of boundary infrared regulator; they can self-intersect; they probe entanglement shadows and orbifold singularities; and they probe the compact space in AdS$_p\\times S^q$. These features imply, according to the python's lunch conjecture, novel qualitative differences between complexity and entanglement in the holographic context. We also find, surprisingly, that extended black brane interiors have a non-extensive complexity; similarly, for multi-boundary wormhole states, the complexity pleateaus after a certain number of boundaries have been included.\n        △ Less",
        "Date": "Submitted 12 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 33,
        "Title": "OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models",
        "Authors": "Authors:\nShuai Wang, \n      \n      Liang Ding, \n      \n      Li Shen, \n      \n      Yong Luo, \n      \n      Bo Du, \n      \n      Dacheng Tao",
        "Abstract": "Abstract:\n      \n        …programming (OOP) in favor of functional programming (FP), e.g., HumanEval and MBPP. To address this, our study introduces a pioneering OOP-focused benchmark, featuring 431 Python programs that encompass essential OOP concepts and features like classes and encapsulation methods. We propose a novel evaluation metric, pass@o, tailored for OOP, enhancing tradit…\n        ▽ More\n\n\n        Advancing automated programming necessitates robust and comprehensive code generation benchmarks, yet current evaluation frameworks largely neglect object-oriented programming (OOP) in favor of functional programming (FP), e.g., HumanEval and MBPP. To address this, our study introduces a pioneering OOP-focused benchmark, featuring 431 Python programs that encompass essential OOP concepts and features like classes and encapsulation methods. We propose a novel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k measures. Our evaluation of 23 leading large language models (LLMs), including both general and code-specialized models, reveals three key insights: 1) pass@o offers a more relevant and comprehensive assessment for OOP code generation; 2) Despite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP compared to models like ChatGPT; 3) The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in this field. Our benchmark and scripts are publicly released at: https://github.com/alphadl/OOP-eval.\n        △ Less",
        "Date": "Submitted 12 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 34,
        "Title": "PyTy: Repairing Static Type Errors in Python",
        "Authors": "Authors:\nYiu Wai Chow, \n      \n      Luca Di Grazia, \n      \n      Michael Pradel",
        "Abstract": "Abstract:\n      \n        …effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a…\n        ▽ More\n\n\n        Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.\n        △ Less",
        "Date": "Submitted 12 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 35,
        "Title": "ML-based Modeling to Predict I/O Performance on Different Storage Sub-systems",
        "Authors": "Authors:\nYiheng Xu, \n      \n      Pranav Sivaraman, \n      \n      Hariharan Devarajan, \n      \n      Kathryn Mohror, \n      \n      Abhinav Bhatele",
        "Abstract": "Abstract:\n      \n        …sub-system for it can be complicated. As a result, adapting parallel applications to use burst buffers efficiently is a trial-and-error process. In this work, we present a Python-based tool called PrismIO that enables programmatic analysis of I/O traces. Using PrismIO, we identify bottlenecks on burst buffers and parallel file systems and explain why certain…\n        ▽ More\n\n\n        Parallel applications can spend a significant amount of time performing I/O on large-scale supercomputers. Fast near-compute storage accelerators called burst buffers can reduce the time a processor spends performing I/O and mitigate I/O bottlenecks. However, determining if a given application could be accelerated using burst buffers is not straightforward even for storage experts. The relationship between an application's I/O characteristics (such as I/O volume, processes involved, etc.) and the best storage sub-system for it can be complicated. As a result, adapting parallel applications to use burst buffers efficiently is a trial-and-error process. In this work, we present a Python-based tool called PrismIO that enables programmatic analysis of I/O traces. Using PrismIO, we identify bottlenecks on burst buffers and parallel file systems and explain why certain I/O patterns perform poorly. Further, we use machine learning to model the relationship between I/O characteristics and burst buffer selections. We run IOR (an I/O benchmark) with various I/O characteristics on different storage systems and collect performance data. We use the data as the input for training the model. Our model can predict if a file of an application should be placed on BBs for unseen IOR scenarios with an accuracy of 94.47% and for four real applications with an accuracy of 95.86%.\n        △ Less",
        "Date": "Submitted 11 January, 2024; v1 submitted 11 December, 2023;\n      originally announced December 2023."
    },
    {
        "ID": 36,
        "Title": "Jupyter widgets and extensions for education and research in computational physics and chemistry",
        "Authors": "Authors:\nDou Du, \n      \n      Taylor J. Baird, \n      \n      Sara Bonella, \n      \n      Giovanni Pizzi",
        "Abstract": "Abstract:\n      \nPython and Jupyter are becoming increasingly popular tools for computational physics and chemistry research and education. Interactive notebooks are a precious tool for creating graphical user interfaces and teaching materials, and Jupyter widgets constitute the core of their interactive functionality. Packages and libraries which offer a broad range of widg…\n        ▽ More\n\n\nPython and Jupyter are becoming increasingly popular tools for computational physics and chemistry research and education. Interactive notebooks are a precious tool for creating graphical user interfaces and teaching materials, and Jupyter widgets constitute the core of their interactive functionality. Packages and libraries which offer a broad range of widgets for general purposes exist, but the lack of specialized widgets for computational physics, chemistry and materials science implies significant time investments for the development of effective Jupyter notebooks for research and education in these domains. Here, we present custom Jupyter widgets that we have developed to target the needs of these research and teaching communities. These widgets constitute high quality interactive graphical components and can be employed, for example, as tools to visualize and manipulate data, or to explore different visual representations of concepts, illuminating the relationships existing between them. In addition, we discuss the JupyterLab extensions that we developed to modify the JupyterLab interface for an enhanced user experience when working with various applications within the targeted scientific domains.\n        △ Less",
        "Date": "Submitted 11 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 37,
        "Title": "An attempt to generate new bridge types from latent space of PixelCNN",
        "Authors": "Authors:\nHongjun Zhang",
        "Abstract": "Abstract:\n      \n        …artificial intelligence technology. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , PixelCNN is constructed and trained. The model can capture the statistical structure of the images and ca…\n        ▽ More\n\n\n        Try to generate new bridge types using generative artificial intelligence technology. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , PixelCNN is constructed and trained. The model can capture the statistical structure of the images and calculate the probability distribution of the next pixel when the previous pixels are given. From the obtained latent space sampling, new bridge types different from the training dataset can be generated. PixelCNN can organically combine different structural components on the basis of human original bridge types, creating new bridge types that have a certain degree of human original ability. Autoregressive models cannot understand the meaning of the sequence, while multimodal models combine regression and autoregressive models to understand the sequence. Multimodal models should be the way to achieve artificial general intelligence in the future.\n        △ Less",
        "Date": "Submitted 11 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 38,
        "Title": "Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs",
        "Authors": "Authors:\nZiyu Li, \n      \n      Donghwan Shin",
        "Abstract": "Abstract:\n      \n        …conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of six programming languages (Python, C++, Java, Go, JavaScript, and Rust). We compare the performance of the LLMs across different types of code mutations and programming languages and analyze the results. We fin…\n        ▽ More\n\n\n        Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.\n  In this paper, we propose a novel method to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. We apply different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. We then use these pairs to test the ability of LLMs to correctly detect the inconsistencies.\n  We propose a new LLM testing method, called Mutation-based Consistency Testing (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of six programming languages (Python, C++, Java, Go, JavaScript, and Rust). We compare the performance of the LLMs across different types of code mutations and programming languages and analyze the results. We find that the LLMs show significant variation in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language.\n        △ Less",
        "Date": "Submitted 11 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 39,
        "Title": "DebugBench: Evaluating Debugging Capability of Large Language Models",
        "Authors": "Authors:\nRunchu Tian, \n      \n      Yining Ye, \n      \n      Yujia Qin, \n      \n      Xin Cong, \n      \n      Yankai Lin, \n      \n      Yinxu Pan, \n      \n      Yesai Wu, \n      \n      Zhiyuan Liu, \n      \n      Maosong Sun",
        "Abstract": "Abstract:\n      \n        …deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two com…\n        ▽ More\n\n\n        Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.\n        △ Less",
        "Date": "Submitted 11 January, 2024; v1 submitted 9 January, 2024;\n      originally announced January 2024."
    },
    {
        "ID": 40,
        "Title": "Rado matroids and a graphical calculus for boundaries of Wilson loop diagrams",
        "Authors": "Authors:\nSusama Agarwala, \n      \n      Colleen Delaney, \n      \n      Karen Yeats",
        "Abstract": "Abstract:\n      \n        …moves to a generalized Wilson loop diagram results in new diagrams that represent boundaries of its associated positroid, without passing through cryptomorphisms. We provide a Python implementation of the graphical calculus and use it to show that the boundaries of positroids associated to ordinary Wilson loop diagram are generated by our diagrammatic moves…\n        ▽ More\n\n\n        We study the boundaries of the positroid cells which arise from N = 4 super Yang Mills theory. Our main tool is a new diagrammatic object which generalizes the Wilson loop diagrams used to represent interactions in the theory. We prove conditions under which these new generalized Wilson loop diagrams correspond to positroids and give an explicit algorithm to calculate the Grassmann necklace of said positroids. Then we develop a graphical calculus operating directly on noncrossing generalized Wilson loop diagrams. In this paradigm, applying diagrammatic moves to a generalized Wilson loop diagram results in new diagrams that represent boundaries of its associated positroid, without passing through cryptomorphisms. We provide a Python implementation of the graphical calculus and use it to show that the boundaries of positroids associated to ordinary Wilson loop diagram are generated by our diagrammatic moves in certain cases.\n        △ Less",
        "Date": "Submitted 10 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 41,
        "Title": "Numerical Method for Modeling Nucleation and Growth of Particles that Prevents Numerical Diffusion",
        "Authors": "Authors:\nA. Khrabry, \n      \n      I. D. Kaganovich, \n      \n      S. Raman, \n      \n      E. Turkoz, \n      \n      D. Graves",
        "Abstract": "Abstract:\n      \n        …of the model to experimental studies with non-monotonic temperature variations leading to particle evaporation. The computational code implementing this numerical method in Python is available upon request.\n        ▽ More\n\n\n        State-of-the-art models for aerosol particle nucleation and growth from a cooling vapor primarily use a nodal method to numerically solve particle growth kinetics. In this method, particles that are smaller than the critical size are omitted from consideration, because they are thermodynamically unfavorable. This omission is based on the assumption that most of the newly formed particles are above the critical size and that the subcritical-size particles are not important to take into account. Due to the nature of the nodal method, it suffers from the numerical diffusion, which can cause an artificial broadening of the cluster size distribution leading to significant overestimation of the number of large-size particles. To address these issues, we propose a more accurate numerical method that explicitly models particles of all sizes, and uses a special numerical scheme that eliminates the numerical diffusion. We extensively compare this novel method to the commonly used nodal solver of the General Dynamics Equation (GDE) for particle growth and demonstrate that it offers GDE solutions with higher accuracy without generating numerical diffusion. Incorporating small subcritical clusters into the solution is crucial for: 1) more precise determination of the entire shape of the particle size distribution function and 2) wider applicability of the model to experimental studies with non-monotonic temperature variations leading to particle evaporation. The computational code implementing this numerical method in Python is available upon request.\n        △ Less",
        "Date": "Submitted 10 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 42,
        "Title": "NE2001p: A Native Python Implementation of the NE2001 Galactic Electron Density Model",
        "Authors": "Authors:\nS. K. Ocker, \n      \n      J. M. Cordes",
        "Abstract": "Abstract:\n      \n        …and the model is routinely used to predict the distances of radio sources lacking independent distance measures. Here we present the open-source package NE2001p, a fully Python implementation of NE2001. The model parameters are identical to NE2001 but the computational architecture is optimized for…\n        ▽ More\n\n\n        The Galactic electron density model NE2001 describes the multicomponent ionized structure of the Milky Way interstellar medium. NE2001 forward models the dispersion and scattering of compact radio sources, including pulsars, fast radio bursts, AGNs, and masers, and the model is routinely used to predict the distances of radio sources lacking independent distance measures. Here we present the open-source package NE2001p, a fully Python implementation of NE2001. The model parameters are identical to NE2001 but the computational architecture is optimized for Python, yielding small (<1%) numerical differences between NE2001p and the Fortran code. NE2001p can be used on the command-line and through Python scripts available on PyPI. Future package releases will include modular extensions aimed at providing short-term improvements to model accuracy, including a modified thick disk scale height and additional clumps and voids. This implementation of NE2001 is a springboard to a next-generation Galactic electron density model now in development.\n        △ Less",
        "Date": "Submitted 10 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 43,
        "Title": "Scalable Gaussian Process Inference with Stan",
        "Authors": "Authors:\nTill Hoffmann, \n      \n      Jukka-Pekka Onnela",
        "Abstract": "Abstract:\n      \n        …Full posterior inference for ten thousand data points is feasible on a laptop in less than 20 seconds. Details on how to get started using the popular interfaces cmdstanpy for Python and cmdstanr for R are provided.\n        ▽ More\n\n\n        Gaussian processes (GPs) are sophisticated distributions to model functional data. Whilst theoretically appealing, they are computationally cumbersome except for small datasets. We implement two methods for scaling GP inference in Stan: First, a general sparse approximation using a directed acyclic dependency graph; second, a fast, exact method for regularly spaced data modeled by GPs with stationary kernels using the fast Fourier transform. Based on benchmark experiments, we offer guidance for practitioners to decide between different methods and parameterizations. We consider two real-world examples to illustrate the package. The implementation follows Stan's design and exposes performant inference through a familiar interface. Full posterior inference for ten thousand data points is feasible on a laptop in less than 20 seconds. Details on how to get started using the popular interfaces cmdstanpy for Python and cmdstanr for R are provided.\n        △ Less",
        "Date": "Submitted 10 January, 2024; v1 submitted 20 January, 2023;\n      originally announced January 2023."
    },
    {
        "ID": 44,
        "Title": "Numerically Computing Finite Temperature Loop Integrals using pySecDec",
        "Authors": "Authors:\nD. Harnett, \n      \n      Siyuan Li, \n      \n      T. G. Steele",
        "Abstract": "Abstract:\n      \n        …phenomena in the Standard Model and extensions, including phase transitions, baryogenesis, and gravitational waves. Methods are developed to enable application of pySecDec (a Python-language-based package designed for numerical calculation of dimensionally-regulated loop integrals) to numerically evaluate finite-temperature loop integrals in the imaginary ti…\n        ▽ More\n\n\n        Finite-temperature quantum field theory provides the foundation for many important phenomena in the Standard Model and extensions, including phase transitions, baryogenesis, and gravitational waves. Methods are developed to enable application of pySecDec (a Python-language-based package designed for numerical calculation of dimensionally-regulated loop integrals) to numerically evaluate finite-temperature loop integrals in the imaginary time (Matsubara) formalism. These methods consist of two main elements: an inverse Wick rotation that converts a finite-temperature loop integral into a form applicable to pySecDec, and asymptotic techniques to regulate and accelerate convergence of the Matsubara frequency summations. Numerical pySecDec evaluation of finite-temperature, two-point and three-point, one-loop topologies for scalar fields is used to illustrate and validate these new methodologies. Advantages of these finite-temperature pySecDec numerical methods are illustrated by the inclusion of multiple mass and external momentum scales.\n        △ Less",
        "Date": "Submitted 10 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 45,
        "Title": "diffSph: a Python tool to compute diffuse signals from dwarf spheroidal galaxies",
        "Authors": "Authors:\nMartin Vollmann, \n      \n      Finn Welzmüller, \n      \n      Lovorka Gajović",
        "Abstract": "Abstract:\n      \n        …composed of Dark Matter, the discovery of these signals could offer valuable insights into understanding the nature of Dark Matter. We present \"diffSph\", a Python tool which in its present version provides fast predictions of such diffuse signals in radio frequencies. It also features a very comprehensive module for the computation of \"J\" and…\n        ▽ More\n\n\n        So far no diffuse emissions in dwarf spheroidal satellites of the Milky Way have ever been observed. Given that dwarf galaxies are predominantly composed of Dark Matter, the discovery of these signals could offer valuable insights into understanding the nature of Dark Matter. We present \"diffSph\", a Python tool which in its present version provides fast predictions of such diffuse signals in radio frequencies. It also features a very comprehensive module for the computation of \"J\" and \"D\" factors that are relevant for indirect Dark Matter detection using gamma rays. Routines are coupled to parton-shower algorithms and Dark Matter halo mass functions from state-of-the-art kinematic fits. This code is also useful for testing generic hypotheses (not necessarily associated with any Dark Matter candidate) about the cosmic-ray electron/positron sources in the dwarf galaxies. The diffSph tool has already been employed in searches for diffuse signals from dwarf spheroidal galaxies using the LOw Frequency ARray (LOFAR).\n        △ Less",
        "Date": "Submitted 10 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 46,
        "Title": "Uniting Gaia and APOGEE to unveil the cosmic chemistry of the Milky Way disc",
        "Authors": "Authors:\nTristan Cantat-Gaudin, \n      \n      Morgan Fouesneau, \n      \n      Hans-Walter Rix, \n      \n      Anthony G. A. Brown, \n      \n      Ronald Drimmel, \n      \n      Alfred Castro-Ginard, \n      \n      Shourya Khanna, \n      \n      Vasily Belokurov, \n      \n      Andrew R. Casey",
        "Abstract": "Abstract:\n      \n        …a metallicity-dependent flaring of the alpha-poor disc. We provide the code for constructing the Gaia selection function we used in this study through the GaiaUnlimited Python package.\n        ▽ More\n\n\n        The spatial distribution of Galactic stars with different chemical abundances encodes information on the processes that drove the formation and evolution of the Milky Way. Survey selection functions are indispensable for analysing astronomical catalogues produced by large-scale surveys. The use of these selection functions in data modelling is more complex when data from different surveys are to be modelled simultaneously. We introduce a procedure for constructing the selection function of a sample of red clump stars that have parallaxes and elemental abundances from the Gaia mission. We separately constructed the selection function of the APOGEE DR17 red clump stars, which depends on very different observables and has a very different spatial coverage. We combined the two surveys and accounted for their joint selection function to provide strong constraints on the radial and vertical density distribution of mono-abundance populations, with Gaia offering a dense coverage of the solar neighbourhood, while APOGEE reaches larger distances near the Galactic plane. We confirm that the radial density profile steepens with increasing metallicity. The combined sample also indicates a metallicity-dependent flaring of the alpha-poor disc. We provide the code for constructing the Gaia selection function we used in this study through the GaiaUnlimited Python package.\n        △ Less",
        "Date": "Submitted 10 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 47,
        "Title": "WARP: The Data Reduction Pipeline for the WINERED spectrograph",
        "Authors": "Authors:\nSatoshi Hamano, \n      \n      Yuji Ikeda, \n      \n      Shogo Otsubo, \n      \n      Haruki Katoh, \n      \n      Kei Fukue, \n      \n      Noriyuki Matsunaga, \n      \n      Daisuke Taniguchi, \n      \n      Hideyo Kawakita, \n      \n      Keiichi Takenaka, \n      \n      Sohei Kondo, \n      \n      Hiroaki Sameshima",
        "Abstract": "Abstract:\n      \n        We present a data reduction pipeline written in Python for data obtained with the near-infrared cross-dispersed echelle spectrograph, WINERED, which yields a 0.91$-$1.35 $μ$m spectrum with the resolving power of $R_{\\text{max}} \\equiv λ/ Δλ= 28,000$ or 70,000 depending on the observing mode. The pipeline was developed to efficiently extract the spectrum from…\n        ▽ More\n\n\n        We present a data reduction pipeline written in Python for data obtained with the near-infrared cross-dispersed echelle spectrograph, WINERED, which yields a 0.91$-$1.35 $μ$m spectrum with the resolving power of $R_{\\text{max}} \\equiv λ/ Δλ= 28,000$ or 70,000 depending on the observing mode. The pipeline was developed to efficiently extract the spectrum from the raw data with high quality. It comprises two modes: the calibration and the science mode. The calibration mode automatically produces the flat-fielding image, bad pixel map, echellogram distortion map and the dispersion solution from the set of the calibration data. Using calibration images and parameters, the science data of astronomical objects can be reduced automatically using the science mode. The science mode is also used for the real-time quick look at the data during observations. An example of the spectra reduced with WARP is presented. The effect of the highly inclined slit image on the spectral resolution is discussed.\n        △ Less",
        "Date": "Submitted 9 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 48,
        "Title": "EV-EcoSim: A grid-aware co-simulation platform for the design and optimization of electric vehicle charging infrastructure",
        "Authors": "Authors:\nEmmanuel Balogun, \n      \n      Elizabeth Buechler, \n      \n      Siddharth Bhela, \n      \n      Simona Onori, \n      \n      Ram Rajagopal",
        "Abstract": "Abstract:\n      \n        …grid transformers, control strategies, and power distribution systems, to perform cost quantification and analyze the impacts of electric vehicle charging on the grid. This python-based platform can run a receding horizon control scheme for real-time operation and a one-shot control scheme for planning problems, with multi-timescale dynamics for different s…\n        ▽ More\n\n\n        To enable the electrification of transportation systems, it is important to understand how technologies such as grid storage, solar photovoltaic systems, and control strategies can aid the deployment of electric vehicle charging at scale. In this work, we present EV-EcoSim, a co-simulation platform that couples electric vehicle charging, battery systems, solar photovoltaic systems, grid transformers, control strategies, and power distribution systems, to perform cost quantification and analyze the impacts of electric vehicle charging on the grid. This python-based platform can run a receding horizon control scheme for real-time operation and a one-shot control scheme for planning problems, with multi-timescale dynamics for different systems to simulate realistic scenarios. We demonstrate the utility of EV-EcoSim through a case study focused on economic evaluation of battery size to reduce electricity costs while considering impacts of fast charging on the power distribution grid. We present qualitative and quantitative evaluations on the battery size in tabulated results. The tabulated results delineate the trade-offs between candidate battery sizing solutions, providing comprehensive insights for decision-making under uncertainty. Additionally, we demonstrate the implications of the battery controller model fidelity on the system costs and show that the fidelity of the battery controller can completely change decisions made when planning an electric vehicle charging site.\n        △ Less",
        "Date": "Submitted 9 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 49,
        "Title": "Statistical mechanical model for crack growth",
        "Authors": "Authors:\nMichael R. Buche, \n      \n      Scott J. Grutzik",
        "Abstract": "Abstract:\n      \n        Analytic relations that describe crack growth are vital for modeling experiments and building a theoretical understanding of fracture. Upon constructing an idealized model system for the crack and applying the principles of statistical thermodynamics, it is possible to formulate the rate of thermally activated crack growth as a function of load, but the result is analytically intractable. Here, an…\n        ▽ More\n\n\n        Analytic relations that describe crack growth are vital for modeling experiments and building a theoretical understanding of fracture. Upon constructing an idealized model system for the crack and applying the principles of statistical thermodynamics, it is possible to formulate the rate of thermally activated crack growth as a function of load, but the result is analytically intractable. Here, an asymptotically correct theory is used to obtain analytic approximations of the crack growth rate from the fundamental theoretical formulation. These crack growth rate relations are compared to those that exist in the literature and are validated with respect to Monte Carlo calculations and experiments. The success of this approach is encouraging for future modeling endeavors that might consider more complicated fracture mechanisms, such as inhomogeneity or a reactive environment.\n        △ Less",
        "Date": "Submitted 9 January, 2024; v1 submitted 1 December, 2022;\n      originally announced December 2022."
    },
    {
        "ID": 50,
        "Title": "OpenSkill: A faster asymmetric multi-team, multiplayer rating system",
        "Authors": "Authors:\nVivek Joshy",
        "Abstract": "Abstract:\n      \n        …provides a more accurate representation of individual player contributions and speeds up the computation of ranks. This paper introduces the OpenSkill library, featuring a Python implementation of the Plackett-Luce model among others, highlighting its performance advantages and predictive accuracy against proprietary systems like TrueSkill. OpenSkill is a va…\n        ▽ More\n\n\n        Assessing and comparing player skill in online multiplayer gaming environments is essential for fair matchmaking and player engagement. Traditional ranking models like Elo and Glicko-2, designed for two-player games, are insufficient for the complexity of multi-player, asymmetric team-based matches. To address this gap, the OpenSkill library offers a suite of sophisticated, fast, and adaptable models tailored for such dynamics. Drawing from Bayesian inference methods, OpenSkill provides a more accurate representation of individual player contributions and speeds up the computation of ranks. This paper introduces the OpenSkill library, featuring a Python implementation of the Plackett-Luce model among others, highlighting its performance advantages and predictive accuracy against proprietary systems like TrueSkill. OpenSkill is a valuable tool for game developers and researchers, ensuring a responsive and fair gaming experience by efficiently adjusting player rankings based on game outcomes. The library's support for time decay and diligent documentation further aid in its practical application, making it a robust solution for the nuanced world of multiplayer ranking systems. This paper also acknowledges areas for future enhancement, such as partial play and contribution weighting, emphasizing the library's ongoing development to meet the evolving needs of online gaming communities.\n        △ Less",
        "Date": "Submitted 9 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 51,
        "Title": "BubbleDet: A Python package to compute functional determinants for bubble nucleation",
        "Authors": "Authors:\nAndreas Ekstedt, \n      \n      Oliver Gould, \n      \n      Joonas Hirvonen",
        "Abstract": "Abstract:\n      \n        We present a Python package, BubbleDet, for computing one-loop functional determinants around spherically symmetric background fields. This gives the next-to-leading order correction to both the vacuum decay rate, at zero temperature, and to the bubble nucleation rate in first-order phase transitions at finite temperature. For predictions of gravitational wa…\n        ▽ More\n\n\n        We present a Python package, BubbleDet, for computing one-loop functional determinants around spherically symmetric background fields. This gives the next-to-leading order correction to both the vacuum decay rate, at zero temperature, and to the bubble nucleation rate in first-order phase transitions at finite temperature. For predictions of gravitational wave signals from cosmological phase transitions, this is expected to remove one of the leading sources of theoretical uncertainty. BubbleDet is applicable to arbitrary scalar potentials and in any dimension up to seven. It has methods for fluctuations of scalar fields, including Goldstone bosons, and for gauge fields, but is limited to cases where the determinant factorises into a product of separate determinants, one for each field degree of freedom. To our knowledge, BubbleDet is the first package dedicated to calculating functional determinants in spherically symmetric background\n        △ Less",
        "Date": "Submitted 9 January, 2024; v1 submitted 29 August, 2023;\n      originally announced August 2023."
    },
    {
        "ID": 52,
        "Title": "Cosmology Ruler Bookmark for Teaching and Outreach Purposes (Pen-and-pencil cosmological ruler calculator for everyone, especially students)",
        "Authors": "Authors:\nHervé Dole",
        "Abstract": "Abstract:\n      \n        …and grad student) in our universities or to secondary schools students we visit. The Cosmology Ruler Bookmark included is ready to print (single- or double-sided). The python script is available on github, allowing changes adapted to everyone's needs for teaching and outreach purposes, including with other cosmologies or applied to other scientific field…\n        ▽ More\n\n\n        Cosmology in general, and relation between redshift and cosmic epoch in particular, is usually obscure to first years university students, secondary students, as well as journalists, politicians and the general public scientists may have interactions with. I identify the need for a simple artifact scientists may give to the public to clarify a few relations between redshift and other physical quantities, more meaningful for a non-scientist audience. This simple bookmark aims at completing previous \"pen-and-pencil cosmological calculator\" nomograms. I created a small, handy, duplicable bookmark with two printed sides, showing the corresponding cosmological values of redshift, age, time, and angular scale (for 1 kpc), using the Planck 2018 cosmology. On the recto, the redshift range of [0.1, 1000] approaches the recombination with a logarithmic scale. On the verso, the redshift range is chosen to be [0, 30] using a linear scale, covering the range of current (and future) detections of galaxies. A few examples are given, illustrating e.g. Planck, JWST or Euclid capabilities and complementarities, time interval non-linearity, properties of galaxies and clusters. This handy bookmark may be printed cheaply and offered to every student in physics (undergrad and grad student) in our universities or to secondary schools students we visit. The Cosmology Ruler Bookmark included is ready to print (single- or double-sided). The python script is available on github, allowing changes adapted to everyone's needs for teaching and outreach purposes, including with other cosmologies or applied to other scientific fields.\n        △ Less",
        "Date": "Submitted 9 January, 2024; v1 submitted 8 January, 2024;\n      originally announced January 2024."
    },
    {
        "ID": 53,
        "Title": "Interior Hulls of Clean Lattice Parallelograms and Continued Fractions",
        "Authors": "Authors:\nGabriel Khan, \n      \n      Mizan R. Khan, \n      \n      Riaz R. Khan, \n      \n      Peng Zhao",
        "Abstract": "Abstract:\n      \n        The interior hull of a lattice polygon is the convex closure of the lattice points in the interior of the polygon. In this paper we give a concrete description of the interior hull of a clean lattice parallelogram. A clean parallelogram in $\\mathbb{R}^2$ is a lattice parallelogram whose boundary contains no lattice points other than its vertices. Using unimodular maps we can identify a clean paral…\n        ▽ More\n\n\n        The interior hull of a lattice polygon is the convex closure of the lattice points in the interior of the polygon. In this paper we give a concrete description of the interior hull of a clean lattice parallelogram. A clean parallelogram in $\\mathbb{R}^2$ is a lattice parallelogram whose boundary contains no lattice points other than its vertices. Using unimodular maps we can identify a clean parallelogram with a parallelogram, $P_{a,n}$, whose vertices are $(0,0), (1,0), (a,n)$ and $(a+1,n)$, with $0<a <n$ and $\\gcd(a,n)=1$. Following Stark's geometric approach to continued fractions we show that the convergents of the continued fraction of $n/a$ (viewed as lattice points) appear in a one-to-two correspondence with the vertices of the interior hull of this parallelogram. Consequently, if the continued fraction of $n/a$ has many partial quotients, then the interior hull of the corresponding parallelogram has many vertices.\n  A pleasing consequence of our work is that we obtain an elementary geometric interpretation of the sum of the partial quotients of the continued fraction of $n/a$. Specifically, it is the difference between the area of the clean parallelogram $P_{a,n}$ and the area of its interior hull.\n        △ Less",
        "Date": "Submitted 8 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 54,
        "Title": "GridPyM: a Python module to handle grid diagrams",
        "Authors": "Authors:\nAgnese Barbensi, \n      \n      Daniele Celoria",
        "Abstract": "Abstract:\n      \n        …diagrams, widely used in theoretical, computational and applied knot theory. Motivated by questions from (bio)-physical knot theory, we introduce GridPyM, a Sage compatible Python module that handles grid diagrams. GridPyM focuses on generating and simplifying grids, and on modelling local transformations between them.\n        ▽ More\n\n\n        Grid diagrams are a combinatorial version of classical link diagrams, widely used in theoretical, computational and applied knot theory. Motivated by questions from (bio)-physical knot theory, we introduce GridPyM, a Sage compatible Python module that handles grid diagrams. GridPyM focuses on generating and simplifying grids, and on modelling local transformations between them.\n        △ Less",
        "Date": "Submitted 8 January, 2024; v1 submitted 13 October, 2022;\n      originally announced October 2022."
    },
    {
        "ID": 55,
        "Title": "Change Point Detection of Events in Molecular Simulations using dupin",
        "Authors": "Authors:\nBrandon L. Butler, \n      \n      Domagoj Fijan, \n      \n      Sharon C. Glotzer",
        "Abstract": "Abstract:\n      \n        …under study. Consequently, methods for event detection lack generality, and those used in one field are not easily used by scientists in other fields. Here we present a new Python-based tool, dupin, that allows for universal event detection from particle trajectory data irrespective of the system details. dupin works by creating a signal representing the sim…\n        ▽ More\n\n\n        Particle tracking is commonly used to study time-dependent behavior in many different types of physical and chemical systems involving constituents that span many length scales, including atoms, molecules, nanoparticles, granular particles, and even larger objects. Behaviors of interest studied using particle tracking information include thermodynamic phase transitions, structural transitions, protein folding, and crystallization. A common challenge in studies of these systems involves change detection. Change point detection discerns when a temporal signal undergoes a change in distribution. These changes can be local or global, instantaneous or prolonged, obvious or subtle. Moreover, system-wide changes marking an interesting physical or chemical phenomenon (e.g. crystallization of a liquid) are often preceded by events (e.g. pre-nucleation clusters) that are localized and can occur anywhere at anytime in the system. For these reasons, detecting events in particle trajectories generated by molecular simulation is challenging and typically accomplished via ad hoc solutions unique to the behavior and system under study. Consequently, methods for event detection lack generality, and those used in one field are not easily used by scientists in other fields. Here we present a new Python-based tool, dupin, that allows for universal event detection from particle trajectory data irrespective of the system details. dupin works by creating a signal representing the simulation and partitioning the signal based on events (changes within the trajectory). This approach allows for studies where manual annotating of event boundaries would require a prohibitive amount of time. Furthermore, dupin can serve as a tool in automated and reproducible workflows. We demonstrate the application of dupin using two examples and discuss its applicability to a wider class of problems.\n        △ Less",
        "Date": "Submitted 8 January, 2024; v1 submitted 22 December, 2023;\n      originally announced December 2023."
    },
    {
        "ID": 56,
        "Title": "Developing Elementary Federated Learning Algorithms Leveraging the ChatGPT",
        "Authors": "Authors:\nMiroslav Popovic, \n      \n      Marko Popovic, \n      \n      Ivan Kastelan, \n      \n      Miodrag Djukic, \n      \n      Ilija Basicevic",
        "Abstract": "Abstract:\n      \n        The Python Testbed for Federated Learning Algorithms is a simple Python FL framework easy to use by ML&AI developers who do not need to be professional programmers, and this paper shows that it is also amenable to emerging AI tools. In this paper, we successfully developed three elementary FL algorithms using the f…\n        ▽ More\n\n\n        The Python Testbed for Federated Learning Algorithms is a simple Python FL framework easy to use by ML&AI developers who do not need to be professional programmers, and this paper shows that it is also amenable to emerging AI tools. In this paper, we successfully developed three elementary FL algorithms using the following three steps process: (i) specify context, (ii) ask ChatGPT to complete server and clients' callback functions, and (iii) verify the generated code.\n        △ Less",
        "Date": "Submitted 8 January, 2024; v1 submitted 7 December, 2023;\n      originally announced December 2023."
    },
    {
        "ID": 57,
        "Title": "Equations of State, Thermodynamics, and Miscibility Curves for Jovian Planet and Giant Exoplanet Evolutionary Models",
        "Authors": "Authors:\nRoberto Tejada Arevalo, \n      \n      Yubo Su, \n      \n      Ankan Sur, \n      \n      Adam Burrows",
        "Abstract": "Abstract:\n      \n        …fidelity than in the past. In this work, we present a set of tools for planetary evolution\\footnote{Available at \\url{https://github.com/Rob685/hhe_eos_misc}} that provides a Python interface for tables of useful thermodynamic quantities, state-of-the-art H-He equations of state, and pressure-dependent immiscibility curves. In particular, for a collection of…\n        ▽ More\n\n\n        The equation of state of hydrogen-helium (H-He) mixtures plays a vital role in the evolution and structure of gas giant planets and exoplanets. Recent equations of state that account for hydrogen-helium interactions, coupled with hydrogen-helium immiscibility curves, can now produce more physical evolutionary models, such as accounting for helium rain with greater fidelity than in the past. In this work, we present a set of tools for planetary evolution\\footnote{Available at \\url{https://github.com/Rob685/hhe_eos_misc}} that provides a Python interface for tables of useful thermodynamic quantities, state-of-the-art H-He equations of state, and pressure-dependent immiscibility curves. In particular, for a collection of independent variable choices, we provide scripts to calculate a variety of thermodynamic derivatives used to model convection and energy transport. This centralized resource is meant to facilitate and consolidate giant planet structural and evolutionary modeling going forward.\n        △ Less",
        "Date": "Submitted 8 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 58,
        "Title": "LASPATED: a Library for the Analysis of SPAtio-TEmporal Discrete data",
        "Authors": "Authors:\nVincent Guigues, \n      \n      Anton Kleywegt, \n      \n      Giovanni Amorim, \n      \n      André Mazal Krauss, \n      \n      Victor Hugo Nascimento",
        "Abstract": "Abstract:\n      \n        …function for calibrating the intensity of the Poisson process. The second method uses additional data to estimate arrival intensity as a function of covariates. We describe a Python package to perform various types of space and time discretization. We also describe two packages for the calibration of the models, one in Matlab and one in C++. We demonstrate t…\n        ▽ More\n\n\n        We describe methods, tools, and a software library called LASPATED, available on GitHub (at https://github.com/vguigues/) to fit models using spatio-temporal data and space-time discretization. A video tutorial for this library is available on YouTube. We consider two types of methods to estimate a non-homogeneous Poisson process in space and time. The methods approximate the arrival intensity function of the Poisson process by discretizing space and time, and estimating arrival intensity as a function of subregion and time interval. With such methods, it is typical that the dimension of the estimator is large relative to the amount of data, and therefore the performance of the estimator can be improved by using additional data. The first method uses additional data to add a regularization term to the likelihood function for calibrating the intensity of the Poisson process. The second method uses additional data to estimate arrival intensity as a function of covariates. We describe a Python package to perform various types of space and time discretization. We also describe two packages for the calibration of the models, one in Matlab and one in C++. We demonstrate the advantages of our methods compared to basic maximum likelihood estimation with simulated and real data. The experiments with real data calibrate models of the arrival process of emergencies to be handled by the Rio de Janeiro emergency medical service.\n        △ Less",
        "Date": "Submitted 8 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 59,
        "Title": "Version Control of Speaker Recognition Systems",
        "Authors": "Authors:\nQuan Wang, \n      \n      Ignacio Lopez Moreno",
        "Abstract": "Abstract:\n      \n        …and hybrid deployment. To compare different strategies with quantitative metrics under various network configurations, we present SpeakerVerSim, an easily-extensible Python-based simulation framework for different server-side deployment strategies of speaker recognition systems.\n        ▽ More\n\n\n        This paper discusses one of the most challenging practical engineering problems in speaker recognition systems - the version control of models and user profiles. A typical speaker recognition system consists of two stages: the enrollment stage, where a profile is generated from user-provided enrollment audio; and the runtime stage, where the voice identity of the runtime audio is compared against the stored profiles. As technology advances, the speaker recognition system needs to be updated for better performance. However, if the stored user profiles are not updated accordingly, version mismatch will result in meaningless recognition results. In this paper, we describe different version control strategies for speaker recognition systems that had been carefully studied at Google from years of engineering practice. These strategies are categorized into three groups according to how they are deployed in the production environment: device-side deployment, server-side deployment, and hybrid deployment. To compare different strategies with quantitative metrics under various network configurations, we present SpeakerVerSim, an easily-extensible Python-based simulation framework for different server-side deployment strategies of speaker recognition systems.\n        △ Less",
        "Date": "Submitted 8 January, 2024; v1 submitted 23 July, 2020;\n      originally announced July 2020."
    },
    {
        "ID": 60,
        "Title": "TextMachina: Seamless Generation of Machine-Generated Text Datasets",
        "Authors": "Authors:\nAreg Mikael Sarvazyan, \n      \n      José Ángel González, \n      \n      Marc Franco-Salvador",
        "Abstract": "Abstract:\n      \n        …tasks. Similar strategies are used to compile these datasets, but no tool currently unifies them. In this scenario, we introduce TextMachina, a modular and extensible Python framework, designed to aid in the creation of high-quality, unbiased datasets to build robust models for MGT-related tasks such as detection, attribution, or boundary detection. It provi…\n        ▽ More\n\n\n        Recent advancements in Large Language Models (LLMs) have led to high-quality Machine-Generated Text (MGT), giving rise to countless new use cases and applications. However, easy access to LLMs is posing new challenges due to misuse. To address malicious usage, researchers have released datasets to effectively train models on MGT-related tasks. Similar strategies are used to compile these datasets, but no tool currently unifies them. In this scenario, we introduce TextMachina, a modular and extensible Python framework, designed to aid in the creation of high-quality, unbiased datasets to build robust models for MGT-related tasks such as detection, attribution, or boundary detection. It provides a user-friendly pipeline that abstracts away the inherent intricacies of building MGT datasets, such as LLM integrations, prompt templating, and bias mitigation. The quality of the datasets generated by TextMachina has been assessed in previous works, including shared tasks where more than one hundred teams trained robust MGT detectors.\n        △ Less",
        "Date": "Submitted 8 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 61,
        "Title": "Toward a comprehensive simulation framework for hypergraphs: a Python-base approach",
        "Authors": "Authors:\nQuoc Chuong Nguyen, \n      \n      Trung Kien Le",
        "Abstract": "Abstract:\n      \n        …dearth presents a barrier to more widespread adaptation of hypergraph computational toolboxes that could enable further research in several areas. Here, we introduce HyperRD, a Python package for hypergraph computation, simulation, and interoperability with other powerful Python packages in graph and hypergraph researc…\n        ▽ More\n\n\n        Hypergraphs, or generalization of graphs such that edges can contain more than two nodes, have become increasingly prominent in understanding complex network analysis. Unlike graphs, hypergraphs have relatively few supporting platforms, and such dearth presents a barrier to more widespread adaptation of hypergraph computational toolboxes that could enable further research in several areas. Here, we introduce HyperRD, a Python package for hypergraph computation, simulation, and interoperability with other powerful Python packages in graph and hypergraph research. Then, we will introduce two models on hypergraph, the general Schelling's model and the SIR model, and simulate them with HyperRD.\n        △ Less",
        "Date": "Submitted 8 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 62,
        "Title": "De-Hallucinator: Iterative Grounding for LLM-Based Code Completion",
        "Authors": "Authors:\nAryaz Eghbali, \n      \n      Michael Pradel",
        "Abstract": "Abstract:\n      \n        …and to the model's initial predictions and adds these references into the prompt. Our evaluation applies the approach to the task of predicting API usages in open-source Python projects. We show that De-Hallucinator consistently improves the predicted code across four state-of-the-art LLMs compared to querying the model only with the code before the curs…\n        ▽ More\n\n\n        Large languages models (LLMs) trained on datasets of publicly available source code have established a new state-of-the-art in code completion. However, these models are mostly unaware of the code that already exists within a specific project, preventing the models from making good use of existing APIs. Instead, LLMs often invent, or \"hallucinate\", non-existent APIs or produce variants of already existing code. Although the API information is available to IDEs, the input size limit of LLMs prevents code completion techniques from including all relevant context into the prompt. This paper presents De-Hallucinator, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt. The approach exploits the observation that LLMs often predict code that resembles the desired completion, but that fails to correctly refer to already existing APIs. De-Hallucinator automatically identifies project-specific API references related to the code prefix and to the model's initial predictions and adds these references into the prompt. Our evaluation applies the approach to the task of predicting API usages in open-source Python projects. We show that De-Hallucinator consistently improves the predicted code across four state-of-the-art LLMs compared to querying the model only with the code before the cursor. In particular, the approach improves the edit distance of the predicted code by 23-51% and the recall of correctly predicted API usages by 24-61% relative to the baseline.\n        △ Less",
        "Date": "Submitted 8 January, 2024; v1 submitted 3 January, 2024;\n      originally announced January 2024."
    },
    {
        "ID": 63,
        "Title": "Boldly Going Where No Benchmark Has Gone Before: Exposing Bias and Shortcomings in Code Generation Evaluation",
        "Authors": "Authors:\nAnkit Yadav, \n      \n      Mayank Singh",
        "Abstract": "Abstract:\n      \n        …proposed to assess the capabilities of existing and emerging models. This study presents a large-scale human evaluation of HumanEval and MBPP, two widely used benchmarks for Python code generation, focusing on their diversity and difficulty. Our findings reveal a significant bias towards a limited number of programming concepts, with negligible or no represe…\n        ▽ More\n\n\n        Motivated by the increasing popularity of code generation from human descriptions using large language models (LLMs), several benchmarks have been proposed to assess the capabilities of existing and emerging models. This study presents a large-scale human evaluation of HumanEval and MBPP, two widely used benchmarks for Python code generation, focusing on their diversity and difficulty. Our findings reveal a significant bias towards a limited number of programming concepts, with negligible or no representation of most concepts. Additionally, we identify a concerningly high proportion of easy programming questions, potentially leading to an overestimation of model performance on code generation tasks.\n        △ Less",
        "Date": "Submitted 8 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 64,
        "Title": "We Need to Talk About Classification Evaluation Metrics in NLP",
        "Authors": "Authors:\nPeter Vickers, \n      \n      Loïc Barrault, \n      \n      Emilio Monti, \n      \n      Nikolaos Aletras",
        "Abstract": "Abstract:\n      \n        …translation. Across these tasks we use a superset of metrics to rank models and find that Informedness best captures the ideal model characteristics. Finally, we release a Python implementation of Informedness following the SciKitLearn classifier format.\n        ▽ More\n\n\n        In Natural Language Processing (NLP) classification tasks such as topic categorisation and sentiment analysis, model generalizability is generally measured with standard metrics such as Accuracy, F-Measure, or AUC-ROC. The diversity of metrics, and the arbitrariness of their application suggest that there is no agreement within NLP on a single best metric to use. This lack suggests there has not been sufficient examination of the underlying heuristics which each metric encodes. To address this we compare several standard classification metrics with more 'exotic' metrics and demonstrate that a random-guess normalised Informedness metric is a parsimonious baseline for task performance. To show how important the choice of metric is, we perform extensive experiments on a wide range of NLP tasks including a synthetic scenario, natural language understanding, question answering and machine translation. Across these tasks we use a superset of metrics to rank models and find that Informedness best captures the ideal model characteristics. Finally, we release a Python implementation of Informedness following the SciKitLearn classifier format.\n        △ Less",
        "Date": "Submitted 8 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 65,
        "Title": "Toward global fits using Higgs STXS data with Lilith",
        "Authors": "Authors:\nDang Bao Nhi Nguyen, \n      \n      Duc Ninh Le, \n      \n      Sabine Kraml, \n      \n      Quang Loc Tran, \n      \n      Van Dung Le",
        "Abstract": "Abstract:\n      \n        In this talk, we present the program Lilith, a python package for constraining new physics from Higgs measurements. We discuss the usage of signal strength results in the latest published version of Lilith, which allows for constraining deviations from SM Higgs couplings through coupling modifiers. Moreover, we discuss the on-going development to include Hig…\n        ▽ More\n\n\n        In this talk, we present the program Lilith, a python package for constraining new physics from Higgs measurements. We discuss the usage of signal strength results in the latest published version of Lilith, which allows for constraining deviations from SM Higgs couplings through coupling modifiers. Moreover, we discuss the on-going development to include Higgs STXS data and SMEFT parametrizations in Lilith with the aim of performing global fits of the ATLAS and CMS data. As we point out, detailed information on Standard Model uncertainties and their correlations is important to enable the proper reuse of the experimental results.\n        △ Less",
        "Date": "Submitted 8 January, 2024; v1 submitted 3 November, 2023;\n      originally announced November 2023."
    },
    {
        "ID": 66,
        "Title": "Assessing AI Detectors in Identifying AI-Generated Code: Implications for Education",
        "Authors": "Authors:\nWei Hung Pan, \n      \n      Ming Jie Chok, \n      \n      Jonathan Leong Shan Wong, \n      \n      Yung Xin Shin, \n      \n      Yeong Shian Poon, \n      \n      Zhou Yang, \n      \n      Chun Yong Chong, \n      \n      David Lo, \n      \n      Mei Kuan Lim",
        "Abstract": "Abstract:\n      \n        …variants. We collected a dataset comprising 5,069 samples, with each sample consisting of a textual description of a coding problem and its corresponding human-written Python solution codes. These samples were obtained from various sources, including 80 from Quescol, 3,264 from Kaggle, and 1,725 from LeetCode. From the dataset, we created 13 sets of code pro…\n        ▽ More\n\n\n        Educators are increasingly concerned about the usage of Large Language Models (LLMs) such as ChatGPT in programming education, particularly regarding the potential exploitation of imperfections in Artificial Intelligence Generated Content (AIGC) Detectors for academic misconduct. In this paper, we present an empirical study where the LLM is examined for its attempts to bypass detection by AIGC Detectors. This is achieved by generating code in response to a given question using different variants. We collected a dataset comprising 5,069 samples, with each sample consisting of a textual description of a coding problem and its corresponding human-written Python solution codes. These samples were obtained from various sources, including 80 from Quescol, 3,264 from Kaggle, and 1,725 from LeetCode. From the dataset, we created 13 sets of code problem variant prompts, which were used to instruct ChatGPT to generate the outputs. Subsequently, we assessed the performance of five AIGC detectors. Our results demonstrate that existing AIGC Detectors perform poorly in distinguishing between human-written code and AI-generated code.\n        △ Less",
        "Date": "Submitted 8 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 67,
        "Title": "Effective Benchmarks for Optical Turbulence Modeling",
        "Authors": "Authors:\nChristopher Jellen, \n      \n      Charles Nelson, \n      \n      Cody Brownell, \n      \n      John Burkhardt",
        "Abstract": "Abstract:\n      \n        …the strength of optical turbulence. However, these metrics are not sufficient for understanding the relative quality of a model. We introduce the \\texttt{otbench} package, a Python package for rigorous development and evaluation of optical turbulence strength prediction models. The package provides a consistent interface for evaluating optical turbulence mod…\n        ▽ More\n\n\n        Optical turbulence presents a significant challenge for communication, directed energy, and imaging systems, especially in the atmospheric boundary layer. Effective modeling of optical turbulence strength is critical for the development and deployment of these systems. The lack of standard evaluation tools, especially long-term data sets, modeling tasks, metrics, and baseline models, prevent effective comparisons between approaches and models. This reduces the ease of reproducing results and contributes to over-fitting on local micro-climates. Performance characterized using evaluation metrics provides some insight into the applicability of a model for predicting the strength of optical turbulence. However, these metrics are not sufficient for understanding the relative quality of a model. We introduce the \\texttt{otbench} package, a Python package for rigorous development and evaluation of optical turbulence strength prediction models. The package provides a consistent interface for evaluating optical turbulence models on a variety of benchmark tasks and data sets. The \\texttt{otbench} package includes a range of baseline models, including statistical, data-driven, and deep learning models, to provide a sense of relative model quality. \\texttt{otbench} also provides support for adding new data sets, tasks, and evaluation metrics. The package is available at \\url{https://github.com/cdjellen/otbench}.\n        △ Less",
        "Date": "Submitted 7 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 68,
        "Title": "CG-Kit: Code Generation Toolkit for Performant and Maintainable Variants of Source Code Applied to Flash-X Hydrodynamics Simulations",
        "Authors": "Authors:\nJohann Rudi, \n      \n      Youngjun Lee, \n      \n      Aidan H. Chadha, \n      \n      Mohamed Wahib, \n      \n      Klaus Weide, \n      \n      Jared P. O'Neal, \n      \n      Anshu Dubey",
        "Abstract": "Abstract:\n      \n        …and maintainability tool chains. Here we present the design of our new tools: parametrized source trees, control flow graphs, and recipes. The tools are implemented in Python. Although the tools are agnostic to the programming language of the source code, we focus on C/C++ and Fortran. Code generation experiments demonstrate the generation of variants of par…\n        ▽ More\n\n\n        CG-Kit is a new code generation toolkit that we propose as a solution for portability and maintainability for scientific computing applications. The development of CG-Kit is rooted in the urgent need created by the shifting landscape of high-performance computing platforms and the algorithmic complexities of a particular large-scale multiphysics application: Flash-X. This combination leads to unique challenges including handling an existing large code base in Fortran and/or C/C++, subdivision of code into a great variety of units supporting a wide range of physics and numerical methods, different parallelization techniques for distributed- and shared-memory systems and accelerator devices, and heterogeneity of computing platforms requiring coexisting variants of parallel algorithms. The challenges demand that developers determine custom abstractions and granularity for code generation. CG-Kit tackles this with standalone tools that can be combined into highly specific and, we argue, highly effective portability and maintainability tool chains. Here we present the design of our new tools: parametrized source trees, control flow graphs, and recipes. The tools are implemented in Python. Although the tools are agnostic to the programming language of the source code, we focus on C/C++ and Fortran. Code generation experiments demonstrate the generation of variants of parallel algorithms: first, multithreaded variants of the basic AXPY operation (scalar-vector addition and vector-vector multiplication) to introduce the application of CG-Kit tool chains; and second, variants of parallel algorithms within a hydrodynamics solver, called Spark, from Flash-X that operates on block-structured adaptive meshes. In summary, code generated by CG-Kit achieves a reduction by over 60% of the original C/C++/Fortran source code.\n        △ Less",
        "Date": "Submitted 6 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 69,
        "Title": "Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design",
        "Authors": "Authors:\nJames T. Meech",
        "Abstract": "Abstract:\n      \n        …the random number generator design using large language model-generated simulations and the Dieharder randomness test suite. We document all the large language model chat logs, Python scripts, Verilog scripts, and simulation results used in the case study. We believe that our method of hardware design generation coupled with the open source silicon 130 nm de…\n        ▽ More\n\n\n        We present a new high-level synthesis methodology for using large language model tools to generate hardware designs. The methodology uses exclusively open-source tools excluding the large language model. As a case study, we use our methodology to generate a permuted congruential random number generator design with a wishbone interface. We verify the functionality and quality of the random number generator design using large language model-generated simulations and the Dieharder randomness test suite. We document all the large language model chat logs, Python scripts, Verilog scripts, and simulation results used in the case study. We believe that our method of hardware design generation coupled with the open source silicon 130 nm design tools will revolutionize application-specific integrated circuit design. Our methodology significantly lowers the bar to entry when building domain-specific computing accelerators for the Internet of Things and proof of concept prototypes for later fabrication in more modern process nodes.\n        △ Less",
        "Date": "Submitted 5 January, 2024; v1 submitted 6 November, 2023;\n      originally announced November 2023."
    },
    {
        "ID": 70,
        "Title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution",
        "Authors": "Authors:\nAlex Gu, \n      \n      Baptiste Rozière, \n      \n      Hugh Leather, \n      \n      Armando Solar-Lezama, \n      \n      Gabriel Synnaeve, \n      \n      Sida I. Wang",
        "Abstract": "Abstract:\n      \n        We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create f…\n        ▽ More\n\n\n        We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create future variation of the benchmark. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval do not show the same improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction, highlighting the gap between open and closed source models. As no model is close to acing CRUXEval, we provide examples of consistent GPT-4 failures on simple programs as a lens into its code reasoning capabilities and areas for improvement.\n        △ Less",
        "Date": "Submitted 5 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 71,
        "Title": "Physics analysis for the HL-LHC: concepts and pipelines in practice with the Analysis Grand Challenge",
        "Authors": "Authors:\nAlexander Held, \n      \n      Elliott Kauffman, \n      \n      Oksana Shadura, \n      \n      Andrew Wightman",
        "Abstract": "Abstract:\n      \n        …needs at the HL-LHC and allow for probing an increased set of functionality. Another focus is the showcase of a reference AGC implementation, which is heavily based on the HEP Python ecosystem and uses modern analysis facilities. The integration of various data delivery strategies is described, resulting in multiple analysis pipelines that are compared to ea…\n        ▽ More\n\n\n        Realistic environments for prototyping, studying and improving analysis workflows are a crucial element on the way towards user-friendly physics analysis at HL-LHC scale. The IRIS-HEP Analysis Grand Challenge (AGC) provides such an environment. It defines a scalable and modular analysis task that captures relevant workflow aspects, ranging from large-scale data processing and handling of systematic uncertainties to statistical inference and analysis preservation. By being based on publicly available Open Data, the AGC provides a point of contact for the broader community. Multiple different implementations of the analysis task that make use of various pipelines and software stacks already exist. This contribution presents an updated AGC analysis task. It features a machine learning component and expanded analysis complexity, including the handling of an extended and more realistic set of systematic uncertainties. These changes both align the AGC further with analysis needs at the HL-LHC and allow for probing an increased set of functionality. Another focus is the showcase of a reference AGC implementation, which is heavily based on the HEP Python ecosystem and uses modern analysis facilities. The integration of various data delivery strategies is described, resulting in multiple analysis pipelines that are compared to each other.\n        △ Less",
        "Date": "Submitted 5 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 72,
        "Title": "Less is More? An Empirical Study on Configuration Issues in Python PyPI Ecosystem",
        "Authors": "Authors:\nYun Peng, \n      \n      Ruida Hu, \n      \n      Ruoke Wang, \n      \n      Cuiyun Gao, \n      \n      Shuqing Li, \n      \n      Michael R. Lyu",
        "Abstract": "Abstract:\n      \nPython is widely used in the open-source community, largely owing to the extensive support from diverse third-party libraries within the PyPI ecosystem. Nevertheless, the utilization of third-party libraries can potentially lead to conflicts in dependencies, prompting researchers to develop dependency conflict detectors. Moreover, endeavors have been made to…\n        ▽ More\n\n\nPython is widely used in the open-source community, largely owing to the extensive support from diverse third-party libraries within the PyPI ecosystem. Nevertheless, the utilization of third-party libraries can potentially lead to conflicts in dependencies, prompting researchers to develop dependency conflict detectors. Moreover, endeavors have been made to automatically infer dependencies. These approaches focus on version-level checks and inference, based on the assumption that configurations of libraries in the PyPI ecosystem are correct. However, our study reveals that this assumption is not universally valid, and relying solely on version-level checks proves inadequate in ensuring compatible run-time environments. In this paper, we conduct an empirical study to comprehensively study the configuration issues in the PyPI ecosystem. Specifically, we propose PyConf, a source-level detector, for detecting potential configuration issues. PyConf employs three distinct checks, targeting the setup, packing, and usage stages of libraries, respectively. To evaluate the effectiveness of the current automatic dependency inference approaches, we build a benchmark called VLibs, comprising library releases that pass all three checks of PyConf. We identify 15 kinds of configuration issues and find that 183,864 library releases suffer from potential configuration issues. Remarkably, 68% of these issues can only be detected via the source-level check. Our experiment results show that the most advanced automatic dependency inference approach, PyEGo, can successfully infer dependencies for only 65% of library releases. The primary failures stem from dependency conflicts and the absence of required libraries in the generated configurations. Based on the empirical results, we derive six findings and draw two implications for open-source developers and future research in automatic dependency inference.\n        △ Less",
        "Date": "Submitted 4 January, 2024; v1 submitted 19 October, 2023;\n      originally announced October 2023."
    },
    {
        "ID": 73,
        "Title": "Dynamic programming by polymorphic semiring algebraic shortcut fusion",
        "Authors": "Authors:\nMax A. Little, \n      \n      Xi He, \n      \n      Ugur Kayas",
        "Abstract": "Abstract:\n      \n        …constraint algebras. We demonstrate the effectiveness of this formalism for some example applications arising in signal processing, bioinformatics and reliability engineering. Python software implementing these algorithms can be downloaded from: http://www.maxlittle.net/software/dppolyalg.zip.\n        ▽ More\n\n\n        Dynamic programming (DP) is an algorithmic design paradigm for the efficient, exact solution of otherwise intractable, combinatorial problems. However, DP algorithm design is often presented in an ad-hoc manner. It is sometimes difficult to justify algorithm correctness. To address this issue, this paper presents a rigorous algebraic formalism for systematically deriving DP algorithms, based on semiring polymorphism. We start with a specification, construct an algorithm to compute the required solution which is self-evidently correct because it exhaustively generates and evaluates all possible solutions meeting the specification. We then derive, through the use of shortcut fusion, an implementation of this algorithm which is both efficient and correct. We also demonstrate how, with the use of semiring lifting, the specification can be augmented with combinatorial constraints, showing how these constraints can be fused with the algorithm. We furthermore demonstrate how existing DP algorithms for a given combinatorial problem can be abstracted from their original context and re-purposed.\n  This approach can be applied to the full scope of combinatorial problems expressible in terms of semirings. This includes, for example: optimal probability and Viterbi decoding, probabilistic marginalization, logical inference, fuzzy sets, differentiable softmax, relational and provenance queries. The approach, building on ideas from the existing literature on constructive algorithmics, exploits generic properties of polymorphic functions, tupling and formal sums and algebraic simplifications arising from constraint algebras. We demonstrate the effectiveness of this formalism for some example applications arising in signal processing, bioinformatics and reliability engineering. Python software implementing these algorithms can be downloaded from: http://www.maxlittle.net/software/dppolyalg.zip.\n        △ Less",
        "Date": "Submitted 4 January, 2024; v1 submitted 4 July, 2021;\n      originally announced July 2021."
    },
    {
        "ID": 74,
        "Title": "Data Integration Framework for Virtual Reality Enabled Digital Twins",
        "Authors": "Authors:\nFlorian Stadtmann, \n      \n      Hary Pirajan Mahalingam, \n      \n      Adil Rasheed",
        "Abstract": "Abstract:\n      \n        …data integration framework for static and real-time data from various sources on the assets and their environment is presented that allows collecting and processing of data in Python and deploying the data in real-time through Unity on different devices, including virtual reality headsets. The integration of data from terrain, weather, and asset geometry is…\n        ▽ More\n\n\n        Digital twins are becoming increasingly popular across many industries for real-time data streaming, processing, and visualization. They allow stakeholders to monitor, diagnose, and optimize assets. Emerging technologies used for immersive visualization, such as virtual reality, open many new possibilities for intuitive access and monitoring of remote assets through digital twins. This is specifically relevant for floating wind farms, where access is often limited. However, the integration of data from multiple sources and access through different devices including virtual reality headsets can be challenging. In this work, a data integration framework for static and real-time data from various sources on the assets and their environment is presented that allows collecting and processing of data in Python and deploying the data in real-time through Unity on different devices, including virtual reality headsets. The integration of data from terrain, weather, and asset geometry is explained in detail. A real-time data stream from the asset to the clients is implemented and reviewed, and instructions are given on the code required to connect Python scripts to any Unity application across devices. The data integration framework is implemented for a digital twin of a floating wind turbine and an onshore wind farm, and the potential for future research is discussed.\n        △ Less",
        "Date": "Submitted 4 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 75,
        "Title": "Fast and Continual Learning for Hybrid Control Policies using Generalized Benders Decomposition",
        "Authors": "Authors:\nXuan Lin",
        "Abstract": "Abstract:\n      \n        …around obstacles. The results show that with significantly less data than previous works, the solver reaches competitive speeds to the off-the-shelf solver Gurobi despite the Python overhead.\n        ▽ More\n\n\n        Hybrid model predictive control with both continuous and discrete variables is widely applicable to robotic control tasks, especially those involving contact with the environment. Due to the combinatorial complexity, the solving speed of hybrid MPC can be insufficient for real-time applications. In this paper, we proposed a hybrid MPC solver based on Generalized Benders Decomposition (GBD). The algorithm enumerates and stores cutting planes online inside a finite buffer. After a short cold-start phase, the stored cuts provide warm-starts for the new problem instances to enhance the solving speed. Despite the disturbance and randomly changing environment, the solving speed maintains. Leveraging on the sparsity of feasibility cuts, we also propose a fast algorithm for Benders master problems. Our solver is validated through controlling a cart-pole system with randomly moving soft contact walls, and a free-flying robot navigating around obstacles. The results show that with significantly less data than previous works, the solver reaches competitive speeds to the off-the-shelf solver Gurobi despite the Python overhead.\n        △ Less",
        "Date": "Submitted 4 January, 2024; v1 submitted 1 January, 2024;\n      originally announced January 2024."
    },
    {
        "ID": 76,
        "Title": "Chebyshev Subdivision and Reduction Methods for Solving Multivariable Systems of Equations",
        "Authors": "Authors:\nErik Parkinson, \n      \n      Kate Wall, \n      \n      Jane Slagle, \n      \n      Daniel Treuhaft, \n      \n      Xander de la Bruere, \n      \n      Samuel Goldrup, \n      \n      Timothy Keith, \n      \n      Peter Call, \n      \n      Tyler J. Jarvis",
        "Abstract": "Abstract:\n      \n        …also work well in higher dimensions. Our tests show that the algorithm outperforms other standard methods on this problem of finding all real zeros in a bounded domain. Our Python implementation of the algorithm is publicly available.\n        ▽ More\n\n\n        We present a new algorithm for finding isolated zeros of a system of real-valued functions in a bounded interval in $\\mathbb{R}^n$. It uses the Chebyshev proxy method combined with a mixture of subdivision, reduction methods, and elimination checks that leverage special properties of Chebyshev polynomials. We prove the method has R-quadratic convergence locally near simple zeros of the system. We also analyze the temporal complexity and the numerical stability of the algorithm and provide numerical evidence in dimensions up to three that the method is both fast and accurate on a wide range of problems. The algorithm should also work well in higher dimensions. Our tests show that the algorithm outperforms other standard methods on this problem of finding all real zeros in a bounded domain. Our Python implementation of the algorithm is publicly available.\n        △ Less",
        "Date": "Submitted 4 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 77,
        "Title": "ModuleGuard:Understanding and Detecting Module Conflicts in Python Ecosystem",
        "Authors": "Authors:\nRuofan Zhu, \n      \n      Xingyu Wang, \n      \n      Chengwei Liu, \n      \n      Zhengzi Xu, \n      \n      Wenbo Shen, \n      \n      Rui Chang, \n      \n      Yang Liu",
        "Abstract": "Abstract:\n      \nPython has become one of the most popular programming languages for software development due to its simplicity, readability, and versatility. As the…\n        ▽ More\n\n\nPython has become one of the most popular programming languages for software development due to its simplicity, readability, and versatility. As the Python ecosystem grows, developers face increasing challenges in avoiding module conflicts, which occur when different packages have the same namespace modules. Unfortunately, existing work has neither investigated the module conflict comprehensively nor provided tools to detect the conflict. Therefore, this paper systematically investigates the module conflict problem and its impact on the Python ecosystem. We propose a novel technique called InstSimulator, which leverages semantics and installation simulation to achieve accurate and efficient module extraction. Based on this, we implement a tool called ModuleGuard to detect module conflicts for the Python ecosystem. For the study, we first collect 97 MC issues, classify the characteristics and causes of these MC issues, summarize three different conflict patterns, and analyze their potential threats. Then, we conducted a large-scale analysis of the whole PyPI ecosystem (4.2 million packages) and GitHub popular projects (3,711 projects) to detect each MC pattern and analyze their potential impact. We discovered that module conflicts still impact numerous TPLs and GitHub projects. This is primarily due to developers' lack of understanding of the modules within their direct dependencies, not to mention the modules of the transitive dependencies. Our work reveals Python's shortcomings in handling naming conflicts and provides a tool and guidelines for developers to detect conflicts.\n        △ Less",
        "Date": "Submitted 4 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 78,
        "Title": "Examining the Challenges in Archiving Instagram",
        "Authors": "Authors:\nRachel Zheng, \n      \n      Michele C. Weigle",
        "Abstract": "Abstract:\n      \n        …in the minority, replayable Instagram mementos exist in public archives and contain valuable data for studying disinformation on Instagram. With that in mind, we developed a Python script to web scrape Instagram mementos. As of August 2023, the Python script can scrape Wayback Machine archives of Instagram account page…\n        ▽ More\n\n\n        To prevent the spread of disinformation on Instagram, we need to study the accounts and content of disinformation actors. However, due to their malicious nature, Instagram often bans accounts that are responsible for spreading disinformation, making these accounts inaccessible from the live web. The only way we can study the content of banned accounts is through public web archives such as the Internet Archive. However, there are many issues present with archiving Instagram pages. Specifically, we focused on the issue that many Wayback Machine Instagram mementos redirect to the Instagram login page. In this study, we determined that mementos of Instagram account pages on the Wayback Machine began redirecting to the Instagram login page in August 2019. We also found that Instagram mementos on Archive.today, Arquivo.pt, and Perma.cc are also not well archived in terms of quantity and quality. Moreover, we were unsuccessful in all our attempts to archive Katy Perry's Instagram account page on Archive.today, Arquivo.pt, and Conifer. Although in the minority, replayable Instagram mementos exist in public archives and contain valuable data for studying disinformation on Instagram. With that in mind, we developed a Python script to web scrape Instagram mementos. As of August 2023, the Python script can scrape Wayback Machine archives of Instagram account pages between November 7, 2012 and June 8, 2018.\n        △ Less",
        "Date": "Submitted 3 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 79,
        "Title": "NIRDust: Probing Hot Dust Emission Around Type 2 AGN Using K-band Spectra",
        "Authors": "Authors:\nGaia Gaspar, \n      \n      Martín Chalela, \n      \n      Juan Cabral, \n      \n      José Alacoria, \n      \n      Damián Mast, \n      \n      Rubén J. Díaz",
        "Abstract": "Abstract:\n      \n        …m).\n  To achieve this, we have developed NIRDust, a Python package for modeling K-band spectra, estimate the dust temperature and characterize the involved uncertainties. We tested synthetic and real spectra in order to check the performance and suitability of the physical model over different types of data.\n  Our tests on synthetic spectra demonstrated that…\n        ▽ More\n\n\n        Hot dust in the proximity of AGNs strongly emits in the Near Infrared producing a red excess that, in type 2 sources, can be modeled to measure its temperature. In the era of high spatial-resolution multi-wavelength data, mapping the hot dust around Supermassive Black Holes is important for the efforts to achieve a complete picture of the dust role and distribution around these compact objects.\n  In this work we propose a methodology to detect the hot dust emission in the proximity of Type 2 AGNs and measure its temperature using K-band spectra ($λ_c$ = 2.2\\,$μ$m).\n  To achieve this, we have developed NIRDust, a Python package for modeling K-band spectra, estimate the dust temperature and characterize the involved uncertainties. We tested synthetic and real spectra in order to check the performance and suitability of the physical model over different types of data.\n  Our tests on synthetic spectra demonstrated that the obtained results are influenced by the signal-to-noise ratio (S/N) of the input spectra. However, we accurately characterized the uncertainties, which remained below $\\sim$150 K for an average S/N per pixel exceeding 20. Applying NIRDust to NGC 5128 (Centaurus A), observed with the Gemini South Telescope, we estimated a dust temperature of 662 and 667 K from Flamingos-2 spectra and 697 and 607 K from GNIRS spectra using two different approaches.\n        △ Less",
        "Date": "Submitted 3 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 80,
        "Title": "The Cytnx Library for Tensor Networks",
        "Authors": "Authors:\nKai-Hsin Wu, \n      \n      Chang-Teng Lin, \n      \n      Ke Hsu, \n      \n      Hao-Ti Hung, \n      \n      Manuel Schneider, \n      \n      Chia-Min Chung, \n      \n      Ying-Jer Kao, \n      \n      Pochung Chen",
        "Abstract": "Abstract:\n      \n        …designed for classical and quantum physics simulations called Cytnx (pronounced as sci-tens). This library provides almost an identical interface and syntax for both C++ and Python, allowing users to effortlessly switch between two languages. Aiming at a quick learning process for new users of tensor network algorithms, the interfaces resemble the popular…\n        ▽ More\n\n\n        We introduce a tensor network library designed for classical and quantum physics simulations called Cytnx (pronounced as sci-tens). This library provides almost an identical interface and syntax for both C++ and Python, allowing users to effortlessly switch between two languages. Aiming at a quick learning process for new users of tensor network algorithms, the interfaces resemble the popular Python scientific libraries like NumPy, Scipy, and PyTorch. Not only multiple global Abelian symmetries can be easily defined and implemented, Cytnx also provides a new tool called Network that allows users to store large tensor networks and perform tensor network contractions in an optimal order automatically. With the integration of cuQuantum, tensor calculations can also be executed efficiently on GPUs. We present benchmark results for tensor operations on both devices, CPU and GPU. We also discuss features and higher-level interfaces to be added in the future.\n        △ Less",
        "Date": "Submitted 3 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 81,
        "Title": "Permanent magnet systems to study the interaction between magnetic nanoparticles and cells in microslide channels",
        "Authors": "Authors:\nLeon Abelmann, \n      \n      Eunheui Gwag, \n      \n      Baeckkyoung Sung",
        "Abstract": "Abstract:\n      \n        …and cross-linked dextran iron-oxide cluster-type particles (MicroMod). Python scripts for magnetic force calculations and particle trajectory modeling as well as source files for 3D prints have been made available so these designs can be easily adapted and optimized for other geometries.\n        ▽ More\n\n\n        We optimized designs of permanent magnet systems to study the effect of magnetic nanoparticles on cell cultures in microslide channels. This produced two designs, one of which is based on a large cylindrical magnet that applies a uniform force density of 6 MN/m$^3$ on soft magnetic iron-oxide spherical nanoparticles at a field strength of over 300 mT. We achieved a force uniformity of better than 14% over the channel area leading to a concentration variation that was below our measurement resolution. The second design was aimed at maximizing the force by using a Halbach array. We indeed increased the force by more than one order of magnitude at force density values over 400 MN/m$^3$, but at the cost of uniformity. However, the latter system can be used to trap magnetic nanoparticles efficiently and to create concentration gradients. We demonstrated both designs by analyzing the effect of magnetic forces on the cell viability of human hepatoma HepG2 cells in the presence of bare Fe$_2$O$_3$ and cross-linked dextran iron-oxide cluster-type particles (MicroMod). Python scripts for magnetic force calculations and particle trajectory modeling as well as source files for 3D prints have been made available so these designs can be easily adapted and optimized for other geometries.\n        △ Less",
        "Date": "Submitted 3 January, 2024; v1 submitted 27 June, 2023;\n      originally announced June 2023."
    },
    {
        "ID": 82,
        "Title": "Raphtory: The temporal graph engine for Rust and Python",
        "Authors": "Authors:\nBen Steer, \n      \n      Naomi Arnold, \n      \n      Cheick Tidiane Ba, \n      \n      Renaud Lambiotte, \n      \n      Haaroon Yousaf, \n      \n      Lucas Jeub, \n      \n      Fabian Murariu, \n      \n      Shivam Kapoor, \n      \n      Pedro Rico, \n      \n      Rachel Chan, \n      \n      Louis Chan, \n      \n      James Alford, \n      \n      Richard G. Clegg, \n      \n      Felix Cuadrado, \n      \n      Matthew Russell Barnes, \n      \n      Peijie Zhong, \n      \n      John N. Pougué Biyong, \n      \n      Alhamza Alnaimi",
        "Abstract": "Abstract:\n      \n        …their structure and evolution; and an extensible GraphQL server for deployment of applications built on top. Raphtory's core engine is built in Rust, for efficiency, with Python interfaces, for ease of use. Raphtory is developed by network scientists, with a background in Physics, Applied Mathematics, Engineering and Computer Science, for use across acad…\n        ▽ More\n\n\n        Raphtory is a platform for building and analysing temporal networks. The library includes methods for creating networks from a variety of data sources; algorithms to explore their structure and evolution; and an extensible GraphQL server for deployment of applications built on top. Raphtory's core engine is built in Rust, for efficiency, with Python interfaces, for ease of use. Raphtory is developed by network scientists, with a background in Physics, Applied Mathematics, Engineering and Computer Science, for use across academia and industry.\n        △ Less",
        "Date": "Submitted 3 January, 2024; v1 submitted 28 June, 2023;\n      originally announced June 2023."
    },
    {
        "ID": 83,
        "Title": "Performance Evaluation of GPS Trajectory Rasterization Methods",
        "Authors": "Authors:\nNecip Enes Gengec, \n      \n      Ergin Tari",
        "Abstract": "Abstract:\n      \n        …GPS trajectory data rasterization using the spatial join functions of QGIS, PostGIS+QGIS, and our iterative spatial structured grid aggregation implementation coded in the Python programming language. Our implementation is also parallelizable, and this parallelization is also included as the fourth method. According to the results of experiment carried out w…\n        ▽ More\n\n\n        The availability of the Global Positioning System (GPS) trajectory data is increasing along with the availability of different GPS receivers and with the increasing use of various mobility services. GPS trajectory is an important data source which is used in traffic density detection, transport mode detection, mapping data inferences with the use of different methods such as image processing and machine learning methods. While the data size increases, efficient representation of this type of data is becoming difficult to be used in these methods. A common approach is the representation of GPS trajectory information such as average speed, bearing, etc. in raster image form and applying analysis methods. In this study, we evaluate GPS trajectory data rasterization using the spatial join functions of QGIS, PostGIS+QGIS, and our iterative spatial structured grid aggregation implementation coded in the Python programming language. Our implementation is also parallelizable, and this parallelization is also included as the fourth method. According to the results of experiment carried out with an example GPS trajectory dataset, QGIS method and PostGIS+QGIS method showed relatively low performance with respect to our method using the metric of total processing time. PostGIS+QGIS method achieved the best results for spatial join though its total performance decreased quickly while test area size increases. On the other hand, both of our methods' performances decrease directly proportional to GPS point. And our methods' performance can be increased proportional to the increase with the number of processor cores and/or with multiple computing clusters.\n        △ Less",
        "Date": "Submitted 3 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 84,
        "Title": "A Simulation-based Approach to Kinematics Analysis of a Quadruped Robot and Prototype Leg Testing",
        "Authors": "Authors:\nAbid Shahriar",
        "Abstract": "Abstract:\n      \n        …of forward kinematics, it measures the position, and joint angles can be calculated using inverse kinematics. Mathematical derivation of the joint angles is derived here, and Python-based simulation has been done to verify and simulate the robot's locomotion. This approach has been tested beneficial over other methods as…\n        ▽ More\n\n\n        Kinematics analysis is a crucial part of multiple joint-enabled robots. A multi-joint enabled robot requires extensive mathematical calculations to be done so the end effector's position can be determined with respect to the other connective joints involved and their respective frames in a specific coordinate system. For a locomotive quadruped robot, it is essential to determine two types of kinematics for the robot's leg position on the coordinate. For the part of forward kinematics, it measures the position, and joint angles can be calculated using inverse kinematics. Mathematical derivation of the joint angles is derived here, and Python-based simulation has been done to verify and simulate the robot's locomotion. This approach has been tested beneficial over other methods as Python-based code is used which makes it easier to do serial communication and therefore it could be deployed in a micro-controller unit to interact with a prototype leg.\n        △ Less",
        "Date": "Submitted 2 January, 2024; v1 submitted 11 December, 2023;\n      originally announced December 2023."
    },
    {
        "ID": 85,
        "Title": "Outlier Ranking in Large-Scale Public Health Streams",
        "Authors": "Authors:\nAnanya Joshi, \n      \n      Tina Townes, \n      \n      Nolan Gormley, \n      \n      Luke Neureiter, \n      \n      Roni Rosenfeld, \n      \n      Bryan Wilder",
        "Abstract": "Abstract:\n      \n        …the best across traditional outlier detection metrics in a human-expert evaluation using public health data streams. Most importantly, experts have used our open-source Python implementation since April 2023 and report identifying outliers worth investigating 9.1x faster than their prior baseline. Other organizations can readily adapt this implementation to…\n        ▽ More\n\n\n        Disease control experts inspect public health data streams daily for outliers worth investigating, like those corresponding to data quality issues or disease outbreaks. However, they can only examine a few of the thousands of maximally-tied outliers returned by univariate outlier detection methods applied to large-scale public health data streams. To help experts distinguish the most important outliers from these thousands of tied outliers, we propose a new task for algorithms to rank the outputs of any univariate method applied to each of many streams. Our novel algorithm for this task, which leverages hierarchical networks and extreme value analysis, performed the best across traditional outlier detection metrics in a human-expert evaluation using public health data streams. Most importantly, experts have used our open-source Python implementation since April 2023 and report identifying outliers worth investigating 9.1x faster than their prior baseline. Other organizations can readily adapt this implementation to create rankings from the outputs of their tailored univariate methods across large-scale streams.\n        △ Less",
        "Date": "Submitted 2 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 86,
        "Title": "A Lagrangian theory for galaxy shape statistics",
        "Authors": "Authors:\nShi-Fan Chen, \n      \n      Nickolas Kokron",
        "Abstract": "Abstract:\n      \n        We formulate the Lagrangian perturbation theory of galaxy intrinsic alignments and compute the resulting auto and cross power spectra of galaxy shapes, densities and matter to 1-loop order. Our model represents a consistent effective-theory description of galaxy shape including the resummation of long-wavelength displacements which damp baryon acoustic oscillations, and includes one linear, three…\n        ▽ More\n\n\n        We formulate the Lagrangian perturbation theory of galaxy intrinsic alignments and compute the resulting auto and cross power spectra of galaxy shapes, densities and matter to 1-loop order. Our model represents a consistent effective-theory description of galaxy shape including the resummation of long-wavelength displacements which damp baryon acoustic oscillations, and includes one linear, three quadratic and two cubic dimensionless bias coefficients at this order, along with counterterms and stochastic contributions whose structure we derive. We compare this Lagrangian model against the three-dimensional helicity spectra of halo shapes measured in N-body simulations by Akitsu et al (2023) and find excellent agreement on perturbative scales while testing a number of more restrictive bias parametrizations. The calculations presented are immediately relevant to analyses of both cosmic shear surveys and spectroscopic shape measurements, and we make a fast FFTLog-based code spinosaurus publicly available with this publication.\n        △ Less",
        "Date": "Submitted 2 January, 2024; v1 submitted 28 September, 2023;\n      originally announced September 2023."
    },
    {
        "ID": 87,
        "Title": "Spiker+: a framework for the generation of efficient Spiking Neural Networks FPGA accelerators for inference at the edge",
        "Authors": "Authors:\nAlessio Carpegna, \n      \n      Alessandro Savino, \n      \n      Stefano Di Carlo",
        "Abstract": "Abstract:\n      \n        …hardware SNN, a library of highly efficient neuron architectures, and a design framework, enabling the development of complex neural network accelerators with few lines of Python code. Spiker+ is tested on two benchmark datasets, the MNIST and the Spiking Heidelberg Digits (SHD). On the MNIST, it demonstrates competitive performance compared to state-of-the-…\n        ▽ More\n\n\n        Including Artificial Neural Networks in embedded systems at the edge allows applications to exploit Artificial Intelligence capabilities directly within devices operating at the network periphery. This paper introduces Spiker+, a comprehensive framework for generating efficient, low-power, and low-area customized Spiking Neural Networks (SNN) accelerators on FPGA for inference at the edge. Spiker+ presents a configurable multi-layer hardware SNN, a library of highly efficient neuron architectures, and a design framework, enabling the development of complex neural network accelerators with few lines of Python code. Spiker+ is tested on two benchmark datasets, the MNIST and the Spiking Heidelberg Digits (SHD). On the MNIST, it demonstrates competitive performance compared to state-of-the-art SNN accelerators. It outperforms them in terms of resource allocation, with a requirement of 7,612 logic cells and 18 Block RAMs (BRAMs), which makes it fit in very small FPGA, and power consumption, draining only 180mW for a complete inference on an input image. The latency is comparable to the ones observed in the state-of-the-art, with 780us/img. To the authors' knowledge, Spiker+ is the first SNN accelerator tested on the SHD. In this case, the accelerator requires 18,268 logic cells and 51 BRAM, with an overall power consumption of 430mW and a latency of 54 us for a complete inference on input data. This underscores the significance of Spiker+ in the hardware-accelerated SNN landscape, making it an excellent solution to deploy configurable and tunable SNN architectures in resource and power-constrained edge applications.\n        △ Less",
        "Date": "Submitted 2 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 88,
        "Title": "TypeEvalPy: A Micro-benchmarking Framework for Python Type Inference Tools",
        "Authors": "Authors:\nAshwin Prasad Shivarpatna Venkatesh, \n      \n      Samkutty Sabu, \n      \n      Jiawei Wang, \n      \n      Amir M. Mir, \n      \n      Li Li, \n      \n      Eric Bodden",
        "Abstract": "Abstract:\n      \n        In light of the growing interest in type inference research for Python, both researchers and practitioners require a standardized process to assess the performance of various type inference techniques. This paper introduces TypeEvalPy, a comprehensive micro-benchmarking framework for evaluating type inference tools. TypeEvalPy contains 154 code snippets with…\n        ▽ More\n\n\n        In light of the growing interest in type inference research for Python, both researchers and practitioners require a standardized process to assess the performance of various type inference techniques. This paper introduces TypeEvalPy, a comprehensive micro-benchmarking framework for evaluating type inference tools. TypeEvalPy contains 154 code snippets with 845 type annotations across 18 categories that target various Python features. The framework manages the execution of containerized tools, transforms inferred types into a standardized format, and produces meaningful metrics for assessment. Through our analysis, we compare the performance of six type inference tools, highlighting their strengths and limitations. Our findings provide a foundation for further research and optimization in the domain of Python type inference.\n        △ Less",
        "Date": "Submitted 2 January, 2024; v1 submitted 28 December, 2023;\n      originally announced December 2023."
    },
    {
        "ID": 89,
        "Title": "An attempt to generate new bridge types from latent space of variational autoencoder",
        "Authors": "Authors:\nHongjun Zhang",
        "Abstract": "Abstract:\n      \n        …(rotation, horizontal scale, vertical scale) to obtain the image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on Python programming language, TensorFlow and Keras deep learning platform framework, variational autoencoder was constructed and trained, and low-dimensional bridge-type latent space that is conve…\n        ▽ More\n\n\n        Try to generate new bridge types using generative artificial intelligence technology. The grayscale images of the bridge facade with the change of component width was rendered by 3dsMax animation software, and then the OpenCV module performed an appropriate amount of geometric transformation (rotation, horizontal scale, vertical scale) to obtain the image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on Python programming language, TensorFlow and Keras deep learning platform framework, variational autoencoder was constructed and trained, and low-dimensional bridge-type latent space that is convenient for vector operations was obtained. Variational autoencoder can combine two bridge types on the basis of the original of human into one that is a new bridge type. Generative artificial intelligence technology can assist bridge designers in bridge-type innovation, and can be used as copilot.\n        △ Less",
        "Date": "Submitted 1 January, 2024; v1 submitted 2 November, 2023;\n      originally announced November 2023."
    },
    {
        "ID": 90,
        "Title": "An attempt to generate new bridge types from latent space of generative adversarial network",
        "Authors": "Authors:\nHongjun Zhang",
        "Abstract": "Abstract:\n      \n        …artificial intelligence technology. Symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge are used . Based on Python programming language, TensorFlow and Keras deep learning platform framework , as well as Wasserstein loss function and Lipschitz constraints, generative adversarial network is cons…\n        ▽ More\n\n\n        Try to generate new bridge types using generative artificial intelligence technology. Symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge are used . Based on Python programming language, TensorFlow and Keras deep learning platform framework , as well as Wasserstein loss function and Lipschitz constraints, generative adversarial network is constructed and trained. From the obtained low dimensional bridge-type latent space sampling, new bridge types with asymmetric structures can be generated. Generative adversarial network can create new bridge types by organically combining different structural components on the basis of human original bridge types. It has a certain degree of human original ability. Generative artificial intelligence technology can open up imagination space and inspire humanity.\n        △ Less",
        "Date": "Submitted 1 January, 2024; \n      originally announced January 2024."
    },
    {
        "ID": 91,
        "Title": "Poker Hand History File Format Specification",
        "Authors": "Authors:\nJuho Kim",
        "Abstract": "Abstract:\n      \n        …covering 11 different variants in the PHH format. Building on our previous work on PokerKit, a premier poker hand simulation tool, we demonstrate the usages of our open-source Python implementation of the PHH parser. The source code of the parser is available on GitHub: https://github.com/uoftcprg/pokerkit\n        ▽ More\n\n\n        This paper introduces the Poker Hand History (PHH) file format, designed to standardize the recording of poker hands across different game variants. Despite poker's widespread popularity in the mainstream culture as a mind sport and its prominence in the field of artificial intelligence (AI) research as a benchmark for imperfect information AI agents, it lacks a consistent format that humans can use to document poker hands across different variants that can also easily be parsed by machines. To address this gap in the literature, we propose the PHH format which provides a concise human-readable machine-friendly representation of hand history that comprehensively captures various details of the hand, ranging from initial game parameters and actions to contextual parameters including but not limited to the venue, players, and time control information. In the supplementary, we provide over 10,000 hands covering 11 different variants in the PHH format. Building on our previous work on PokerKit, a premier poker hand simulation tool, we demonstrate the usages of our open-source Python implementation of the PHH parser. The source code of the parser is available on GitHub: https://github.com/uoftcprg/pokerkit\n        △ Less",
        "Date": "Submitted 1 January, 2024; v1 submitted 18 December, 2023;\n      originally announced December 2023."
    },
    {
        "ID": 92,
        "Title": "Data Dialogue with ChatGPT: Using Code Interpreter to Simulate and Analyse Experimental Data",
        "Authors": "Authors:\nAndrew Low, \n      \n      Z. Yasemin Kalender",
        "Abstract": "Abstract:\n      \n        …to complete an introductory mechanics laboratory activity by using Code Interpreter, a recent plugin that allows users to generate and analyse data by writing and running Python code `behind the scenes'. By uploading a common `spring constant' lab activity using Code Interpreter, we investigate the ability of ChatGPT to interpret the activity, genera…\n        ▽ More\n\n\n        Artificial Intelligence (AI) has the potential to fundamentally change the educational landscape. So far, much of the physics education research relating to AI has focused on lecture-based assessment and the ability of ChatGPT to answer conceptual surveys and traditional exam-style questions. In this study, we shift the focus by investigating ChatGPT's ability to complete an introductory mechanics laboratory activity by using Code Interpreter, a recent plugin that allows users to generate and analyse data by writing and running Python code `behind the scenes'. By uploading a common `spring constant' lab activity using Code Interpreter, we investigate the ability of ChatGPT to interpret the activity, generate realistic model data, produce a line-fit, and calculate the reduced chi square statistic. By analysing our interactions with ChatGPT, along with the Python code generated by Code Interpreter, we assess how the quality and accuracy of ChatGPT's responses depends on different levels of prompt detail. We find that although ChatGPT is capable of completing the lab activity and generating plausible-looking data, the quality of the output is highly dependent on the detail and specificity of the text prompts provided. We find that the data generation process adopted by ChatGPT in this study leads to heteroscedasticity in the simulated data, which may be difficult for novice learners to spot. We also find that when real experimental data is uploaded via Code Interpreter, ChatGPT is capable of correctly plotting and fitting the data, calculating the spring constant and associated uncertainty, and calculating the reduced chi square statistic. This work offers new insights into the capabilities of Code Interpreter within a laboratory setting and highlights a variety of text-prompt strategies for the effective use of Code Interpreter in a lab context.\n        △ Less",
        "Date": "Submitted 31 December, 2023; v1 submitted 21 November, 2023;\n      originally announced November 2023."
    },
    {
        "ID": 93,
        "Title": "Parallel sampling of decomposable graphs using Markov chain on junction trees",
        "Authors": "Authors:\nMohamad Elmasri",
        "Abstract": "Abstract:\n      \n        …the single-move variate, and outperforms current state-of-the-arts methods in terms of accuracy and computational efficiency. The implementation of our work is available in the Python package parallelDG.\n        ▽ More\n\n\n        Bayesian inference for undirected graphical models is mostly restricted to the class of decomposable graphs, as they enjoy a rich set of properties making them amenable to high-dimensional problems. While parameter inference is straightforward in this setup, inferring the underlying graph is a challenge driven by the computational difficulty in exploring the space of decomposable graphs. This work makes two contributions to address this problem. First, we provide sufficient and necessary conditions for when multi-edge perturbations maintain decomposability of the graph. Using these, we characterize a simple class of partitions that efficiently classify all edge perturbations by whether they maintain decomposability. Second, we propose a novel parallel non-reversible Markov chain Monte Carlo sampler for distributions over junction tree representations of the graph. At every step, the parallel sampler executes simultaneously all edge perturbations within a partition. Through simulations, we demonstrate the efficiency of our new edge perturbation conditions and class of partitions. We find that our parallel sampler yields improved mixing properties in comparison to the single-move variate, and outperforms current state-of-the-arts methods in terms of accuracy and computational efficiency. The implementation of our work is available in the Python package parallelDG.\n        △ Less",
        "Date": "Submitted 31 December, 2023; v1 submitted 5 September, 2022;\n      originally announced September 2022."
    },
    {
        "ID": 94,
        "Title": "A Toolbox for Modelling Engagement with Educational Videos",
        "Authors": "Authors:\nYuxiang Qiu, \n      \n      Karim Djemili, \n      \n      Denis Elezi, \n      \n      Aaneel Shalman, \n      \n      María Pérez-Ortiz, \n      \n      Emine Yilmaz, \n      \n      John Shawe-Taylor, \n      \n      Sahan Bulathwela",
        "Abstract": "Abstract:\n      \n        …(AI), personalising education to a global population could be a cornerstone of new educational systems in the future. This work presents the PEEKC dataset and the TrueLearn Python library, which contains a dataset and a series of online learner state models that are essential to facilitate research on learner engagement modelling.TrueLearn family of models w…\n        ▽ More\n\n\n        With the advancement and utility of Artificial Intelligence (AI), personalising education to a global population could be a cornerstone of new educational systems in the future. This work presents the PEEKC dataset and the TrueLearn Python library, which contains a dataset and a series of online learner state models that are essential to facilitate research on learner engagement modelling.TrueLearn family of models was designed following the \"open learner\" concept, using humanly-intuitive user representations. This family of scalable, online models also help end-users visualise the learner models, which may in the future facilitate user interaction with their models/recommenders. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytics practitioners. The experiments show the utility of both the dataset and the library with predictive performance significantly exceeding comparative baseline models. The dataset contains a large amount of AI-related educational videos, which are of interest for building and validating AI-specific educational recommenders.\n        △ Less",
        "Date": "Submitted 30 December, 2023; \n      originally announced January 2024."
    },
    {
        "ID": 95,
        "Title": "Laboratory Experiments of Model-based Reinforcement Learning for Adaptive Optics Control",
        "Authors": "Authors:\nJalo Nousiainen, \n      \n      Byron Engler, \n      \n      Markus Kasper, \n      \n      Chang Rajani, \n      \n      Tapio Helin, \n      \n      Cédric T. Heritier, \n      \n      Sascha P. Quanz, \n      \n      Adrian M. Glauser",
        "Abstract": "Abstract:\n      \n        …and self-calibrating aspects of the method. The new implementation on GHOST running PyTorch introduces only around 700 microseconds in addition to hardware, pipeline, and Python interface latency. We open-source well-documented code for the implementation and specify the requirements for the RTC pipeline. We also discuss the important hyperparameters of the…\n        ▽ More\n\n\n        Direct imaging of Earth-like exoplanets is one of the most prominent scientific drivers of the next generation of ground-based telescopes. Typically, Earth-like exoplanets are located at small angular separations from their host stars, making their detection difficult. Consequently, the adaptive optics (AO) system's control algorithm must be carefully designed to distinguish the exoplanet from the residual light produced by the host star.\n  A new promising avenue of research to improve AO control builds on data-driven control methods such as Reinforcement Learning (RL). RL is an active branch of the machine learning research field, where control of a system is learned through interaction with the environment. Thus, RL can be seen as an automated approach to AO control, where its usage is entirely a turnkey operation. In particular, model-based reinforcement learning (MBRL) has been shown to cope with both temporal and misregistration errors. Similarly, it has been demonstrated to adapt to non-linear wavefront sensing while being efficient in training and execution.\n  In this work, we implement and adapt an RL method called Policy Optimization for AO (PO4AO) to the GHOST test bench at ESO headquarters, where we demonstrate a strong performance of the method in a laboratory environment. Our implementation allows the training to be performed parallel to inference, which is crucial for on-sky operation. In particular, we study the predictive and self-calibrating aspects of the method. The new implementation on GHOST running PyTorch introduces only around 700 microseconds in addition to hardware, pipeline, and Python interface latency. We open-source well-documented code for the implementation and specify the requirements for the RTC pipeline. We also discuss the important hyperparameters of the method, the source of the latency, and the possible paths for a lower latency implementation.\n        △ Less",
        "Date": "Submitted 30 December, 2023; \n      originally announced January 2024."
    },
    {
        "ID": 96,
        "Title": "Is Knowledge All Large Language Models Needed for Causal Reasoning?",
        "Authors": "Authors:\nHengrui Cai, \n      \n      Shengjie Liu, \n      \n      Rui Song",
        "Abstract": "Abstract:\n      \n        This paper explores the causal reasoning of large language models (LLMs) to enhance their interpretability and reliability in advancing artificial intelligence. Despite the proficiency of LLMs in a range of tasks, their potential for understanding causality requires further exploration. We propose a novel causal attribution model that utilizes \"do-operators\" for constructing counterfactual scenari…\n        ▽ More\n\n\n        This paper explores the causal reasoning of large language models (LLMs) to enhance their interpretability and reliability in advancing artificial intelligence. Despite the proficiency of LLMs in a range of tasks, their potential for understanding causality requires further exploration. We propose a novel causal attribution model that utilizes \"do-operators\" for constructing counterfactual scenarios, allowing us to systematically quantify the influence of input numerical data and LLMs' pre-existing knowledge on their causal reasoning processes. Our newly developed experimental setup assesses LLMs' reliance on contextual information and inherent knowledge across various domains. Our evaluation reveals that LLMs' causal reasoning ability depends on the context and domain-specific knowledge provided, and supports the argument that \"knowledge is, indeed, what LLMs principally require for sound causal reasoning\". On the contrary, in the absence of knowledge, LLMs still maintain a degree of causal reasoning using the available numerical data, albeit with limitations in the calculations.\n        △ Less",
        "Date": "Submitted 29 December, 2023; \n      originally announced January 2024."
    },
    {
        "ID": 97,
        "Title": ": A Python Package for Measuring The Composition of Complex Datasets",
        "Authors": "Authors:\nPhuc Nguyen, \n      \n      Rohit Arora, \n      \n      Elliot D. Hill, \n      \n      Jasper Braun, \n      \n      Alexandra Morgan, \n      \n      Liza M. Quintana, \n      \n      Gabrielle Mazzoni, \n      \n      Ghee Rye Lee, \n      \n      Rima Arnaout, \n      \n      Ramy Arnaout",
        "Abstract": "Abstract:\n      \n        …and between-element similarities. Although these have been available in the R and Julia programming languages for other applications, they have not been as readily available in Python, which is widely used for machine learning, and are not easily applied to machine-learning-sized datasets without special coding considerations. To address these issues, we dev…\n        ▽ More\n\n\n        Machine-learning datasets are typically characterized by measuring their size and class balance. However, there exists a richer and potentially more useful set of measures, termed diversity measures, that incorporate elements' frequencies and between-element similarities. Although these have been available in the R and Julia programming languages for other applications, they have not been as readily available in Python, which is widely used for machine learning, and are not easily applied to machine-learning-sized datasets without special coding considerations. To address these issues, we developed $\\textit{greylock}$, a Python package that calculates diversity measures and is tailored to large datasets. $\\textit{greylock}$ can calculate any of the frequency-sensitive measures of Hill's D-number framework, and going beyond Hill, their similarity-sensitive counterparts (Greylock is a mountain). $\\textit{greylock}$ also outputs measures that compare datasets (beta diversities). We first briefly review the D-number framework, illustrating how it incorporates elements' frequencies and between-element similarities. We then describe $\\textit{greylock}$'s key features and usage. We end with several examples - immunomics, metagenomics, computational pathology, and medical imaging - illustrating $\\textit{greylock}$'s applicability across a range of dataset types and fields.\n        △ Less",
        "Date": "Submitted 29 December, 2023; \n      originally announced January 2024."
    },
    {
        "ID": 98,
        "Title": "Data Augmentation for Supervised Graph Outlier Detection with Latent Diffusion Models",
        "Authors": "Authors:\nKay Liu, \n      \n      Hengrui Zhang, \n      \n      Ziqing Hu, \n      \n      Fangxin Wang, \n      \n      Philip S. Yu",
        "Abstract": "Abstract:\n      \n        …demonstrated the generation quality of our synthetic data. To foster accessibility and reproducibility, we encapsulate GODM into a plug-and-play package and release it at the Python Package Index (PyPI).\n        ▽ More\n\n\n        Graph outlier detection is a prominent task of research and application in the realm of graph neural networks. It identifies the outlier nodes that exhibit deviation from the majority in the graph. One of the fundamental challenges confronting supervised graph outlier detection algorithms is the prevalent issue of class imbalance, where the scarcity of outlier instances compared to normal instances often results in suboptimal performance. Conventional methods mitigate the imbalance by reweighting instances in the estimation of the loss function, assigning higher weights to outliers and lower weights to inliers. Nonetheless, these strategies are prone to overfitting and underfitting, respectively. Recently, generative models, especially diffusion models, have demonstrated their efficacy in synthesizing high-fidelity images. Despite their extraordinary generation quality, their potential in data augmentation for supervised graph outlier detection remains largely underexplored.\n  To bridge this gap, we introduce GODM, a novel data augmentation for mitigating class imbalance in supervised Graph Outlier detection with latent Diffusion Models. Specifically, our proposed method consists of three key components: (1) Variantioanl Encoder maps the heterogeneous information inherent within the graph data into a unified latent space. (2) Graph Generator synthesizes graph data that are statistically similar to real outliers from latent space, and (3) Latent Diffusion Model learns the latent space distribution of real organic data by iterative denoising. Extensive experiments conducted on multiple datasets substantiate the effectiveness and efficiency of GODM. The case study further demonstrated the generation quality of our synthetic data. To foster accessibility and reproducibility, we encapsulate GODM into a plug-and-play package and release it at the Python Package Index (PyPI).\n        △ Less",
        "Date": "Submitted 29 December, 2023; \n      originally announced December 2023."
    },
    {
        "ID": 99,
        "Title": "Holographic Weak Measurement",
        "Authors": "Authors:\nXinyu Sun, \n      \n      Shao-Kai Jian",
        "Abstract": "Abstract:\n      \n        …spacetime and a black hole spacetime that are separated by the brane. Although the measurement is irrelevant in the later phase, the post-measurement geometry can realize a Python's lunch.\n        ▽ More\n\n\n        In this paper, we study a holographic description of weak measurements in conformal field theories (CFTs). Weak measurements can be viewed as a soft projection that interpolates between an identity operator and a projection operator, and can induce an effective central charge distinct from the unmeasured CFT. We model the weak measurement by an interface brane, separating different geometries dual to the post-measurement state and the unmeasured CFT, respectively. In an infinite system, the weak measurement is related to ICFT via a spacetime rotation. We find that the holographic entanglement entropy with twist operators located on the defect is consistent in both calculations for ICFT and weak measurements. We additionally calculate the boundary entropy via holographic entanglement as well as partition function. In a finite system, the weak measurement can lead to a rich phase diagram: for marginal measurements the emergent brane separates two AdS geometries, while for irrelevant measurements the post-measurement geometry features an AdS spacetime and a black hole spacetime that are separated by the brane. Although the measurement is irrelevant in the later phase, the post-measurement geometry can realize a Python's lunch.\n        △ Less",
        "Date": "Submitted 28 December, 2023; v1 submitted 27 September, 2023;\n      originally announced September 2023."
    },
    {
        "ID": 100,
        "Title": "CluBear: A Subsampling Package for Interactive Statistical Analysis with Massive Data on A Single Machine",
        "Authors": "Authors:\nKe Xu, \n      \n      Yingqiu Zhu, \n      \n      Yijing Liu, \n      \n      Hansheng Wang",
        "Abstract": "Abstract:\n      \n        This article introduces CluBear, a Python-based open-source package for interactive massive data analysis. The key feature of CluBear is that it enables users to conduct convenient and interactive statistical analysis of massive data with only a traditional single-computer system. Thus, CluBear provides a cost-effective solution when mining large-scale datas…\n        ▽ More\n\n\n        This article introduces CluBear, a Python-based open-source package for interactive massive data analysis. The key feature of CluBear is that it enables users to conduct convenient and interactive statistical analysis of massive data with only a traditional single-computer system. Thus, CluBear provides a cost-effective solution when mining large-scale datasets. In addition, the CluBear package integrates many commonly used statistical and graphical tools, which are useful for most commonly encountered data analysis tasks.\n        △ Less",
        "Date": "Submitted 28 December, 2023; \n      originally announced December 2023."
    }
]